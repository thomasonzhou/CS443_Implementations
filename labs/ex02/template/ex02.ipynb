{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:\n",
    "\n",
    "## Question responses\n",
    "The 1st column in X represents a weight offset that is not directly associated with height.\n",
    "The 2nd column in X represents a height weight that relates to the output weight.\n",
    "\n",
    "Each row represents a sample, corresponding to a person.\n",
    "\n",
    "We have ones to represent the weights with no associated input variable.\n",
    "\n",
    "For 3 people, Y would be 3x1, and X would be 3x2.\n",
    "$$X_{32}$$ would represent the model weight associated with the height of the third person.\n",
    "\n",
    "$$\\mathcal{L} = \\sum\\lvert e \\rvert^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2694.4833658870843\n"
     ]
    }
   ],
   "source": [
    "def calculate_mse(e):\n",
    "    return np.mean(e**2) / 2\n",
    "\n",
    "def calculate_mae(e):\n",
    "    return np.mean(np.abs(e))\n",
    "\n",
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # print(y, tx, w)\n",
    "    # print((y - (tx.dot(w.T))))\n",
    "    e = y - tx.dot(w.T)\n",
    "    return calculate_mse(e)\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "w = np.array([1, 2])\n",
    "loss = compute_loss(y, tx, w)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    for i, row in enumerate(grid_w0):\n",
    "        for j, col in enumerate(grid_w1):\n",
    "            w = np.array([row, col])\n",
    "            losses[i][j] = compute_loss(y, tx, w)\n",
    "    \n",
    "#     grid_w0 = grid_w0[:, np.newaxis]\n",
    "#     grid_w1 = grid_w1[np.newaxis, :]\n",
    "    \n",
    "#     w = [grid_w0[:], grid_w1[:]]\n",
    "    \n",
    "#     w.reshape()\n",
    "    \n",
    "#     pred_y = tx @ w\n",
    "#     diff = y[:,np.newaxis] - pred_y\n",
    "#     losses1 = np.sum(np.square(diff), axis = -1)\n",
    "#     print(losses, losses1)\n",
    "    \n",
    "\n",
    "    # ***************************************************\n",
    "    return losses\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=18.793541019523236, w0*=71.42857142857142, w1*=15.306122448979579, execution time=19.570 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=50)\n",
    "#upped to 50 from 10\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line of best fit is skewed to be a reasonable fit of the data, however there is clearly a better solution available.\n",
    "Maybe the granularity of the grid is not good enough? This is just based on the next steps, since it seems like there aren't many significant outliers that pull the left side of the graph downwards.\n",
    "\n",
    "After upping the granularity of the grid search from steps of 50 to steps of 10, the line of best fit modelled the trend much better. \n",
    "\n",
    "To obtain a better fit, a more accurate fit is optimal. However, this comes at the cost of increasingly high computational time with significant reductions in value.\n",
    "e.g. 12 seconds for 50 divisions, 52 seconds for 100 divisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.5 16.5]\n"
     ]
    }
   ],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    N = y.shape[0]\n",
    "    \n",
    "    error = y - (tx @ w)\n",
    "    return -(1/N) * (tx.T @ error)\n",
    "    # ***************************************************\n",
    "\n",
    "Y = np.array([1,2]).T\n",
    "Xt = np.array([[1, 1], [3, 3]]).T\n",
    "W = np.array([1,2]).T\n",
    "print(compute_gradient(Y, Xt, W))\n",
    "assert(np.array_equal(compute_gradient(Y, Xt, W), np.array([5.5, 16.5])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients tell us how far we are from our \"desired\" model. Larger values are caused by large deviations from the expected output, which in turn influence updates by changing the relative speed that a parameter increases or decreases relative to the other adjustable parameters.\n",
    "\n",
    "Imagining a quadratic function, as we approach its minimum the slope becomes more and more flat, until it approaches a value infintesimaly close to 0. \n",
    "\n",
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        gradient = compute_gradient(y, tx, w)\n",
    "\n",
    "        loss = compute_loss(y, tx, w)\n",
    "        # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        w = w - gamma * gradient\n",
    "        # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:\n",
    "\n",
    "d) Much slower convergence rate with lower gamma, as expected. If gamma is too low, the local minimum is not reached within 50 iterations. Although this example was simple and therefore not very computationally intense, larger scale models leveraging advanced loss techniques will experience a significant performance hit at low training rates (~0.1).\n",
    "\n",
    "At high training rates (~>2) the model jumps around and cannot find the local minimum. In extreme cases it even escapes the scope of data points.\n",
    "\n",
    "When weight values were close to the ideal values, convergence time was relatively similar within a certain radius. Otherwise, significant distances from the ideal values resulted in longer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2792.2367127591674, w0=51.30574540147363, w1=9.435798704492282\n",
      "GD iter. 1/49: loss=265.30246210896007, w0=66.69746902191572, w1=12.266538315840002\n",
      "GD iter. 2/49: loss=37.87837955044118, w0=71.31498610804834, w1=13.11576019924433\n",
      "GD iter. 3/49: loss=17.41021212017447, w0=72.70024123388814, w1=13.370526764265632\n",
      "GD iter. 4/49: loss=15.568077051450455, w0=73.11581777164007, w1=13.446956733772023\n",
      "GD iter. 5/49: loss=15.402284895265295, w0=73.24049073296565, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=15.38736360120863, w0=73.27789262136334, w1=13.476764421879516\n",
      "GD iter. 7/49: loss=15.38602068474353, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=15.385899822261674, w0=73.29247935783842, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=15.385888944638305, w0=73.29348920882515, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=15.3858879656522, w0=73.29379216412117, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=15.385887877543452, w0=73.29388305070998, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=15.385887869613665, w0=73.29391031668662, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=15.385887868899983, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=15.38588786883575, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=15.385887868829974, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=15.38588786882945, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=15.385887868829403, w0=73.29392197370962, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=15.3858878688294, w0=73.29392199358652, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=15.385887868829403, w0=73.2939219995496, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=15.385887868829398, w0=73.29392200133852, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=15.3858878688294, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=15.3858878688294, w0=73.29392200203618, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=15.3858878688294, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=15.3858878688294, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=15.385887868829398, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=15.3858878688294, w0=73.29392200210464, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=15.3858878688294, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=15.3858878688294, w0=73.29392200210513, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=15.3858878688294, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=15.3858878688294, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=1.454 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "# gamma = 0.1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "# w_initial = np.array([-1000, 1000])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcee1fe3a2064352b36a88b1671ec980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "\n",
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    return compute_gradient(y, tx, w)\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        for y_shuffle, tx_shuffle in batch_iter(y, tx, batch_size, 1, shuffle=True):\n",
    "            # if y_shuffle is None:\n",
    "            #     break\n",
    "            \n",
    "            gradient = compute_stoch_gradient(y_shuffle, tx_shuffle, w)\n",
    "            w = w - gamma * gradient\n",
    "            loss=compute_loss(y, tx, w)\n",
    "            \n",
    "            ws.append(w)\n",
    "            losses.append(loss)\n",
    "        \n",
    "        # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2483.3932275151465, w0=5.869856001933149, w1=-6.268958489837312\n",
      "SGD iter. 1/49: loss=1752.4009429771077, w0=14.96896766570088, w1=4.980900054843806\n",
      "SGD iter. 2/49: loss=1348.8095193445433, w0=21.718177956321426, w1=10.873970875427077\n",
      "SGD iter. 3/49: loss=1094.784877057013, w0=26.84538125201682, w1=12.3260051221285\n",
      "SGD iter. 4/49: loss=878.8873223464474, w0=31.761391436353883, w1=14.912113831305545\n",
      "SGD iter. 5/49: loss=736.6272345189662, w0=35.37135058354825, w1=11.391346972546641\n",
      "SGD iter. 6/49: loss=630.5127549680649, w0=38.816871527994884, w1=7.030935434338312\n",
      "SGD iter. 7/49: loss=476.1912287532082, w0=43.00306002970147, w1=11.461207360052594\n",
      "SGD iter. 8/49: loss=405.08029943928426, w0=45.96778769725684, w1=7.763839177620563\n",
      "SGD iter. 9/49: loss=294.6338446113604, w0=49.66548831829809, w1=13.919070074039467\n",
      "SGD iter. 10/49: loss=247.96729507989585, w0=51.76988611877588, w1=14.850366919272055\n",
      "SGD iter. 11/49: loss=226.1163467248788, w0=52.897942296346116, w1=15.817431313375145\n",
      "SGD iter. 12/49: loss=172.104340015803, w0=55.59685604304242, w1=12.978952278298198\n",
      "SGD iter. 13/49: loss=143.32743141796936, w0=57.35342253812668, w1=12.14421105454018\n",
      "SGD iter. 14/49: loss=121.9987178322897, w0=58.69258427134939, w1=13.642796460349263\n",
      "SGD iter. 15/49: loss=105.14754506884908, w0=59.90943993755612, w1=12.86411982310703\n",
      "SGD iter. 16/49: loss=90.62935552185876, w0=61.25224579519752, w1=15.821713595119478\n",
      "SGD iter. 17/49: loss=65.5328319425642, w0=63.3835591920628, w1=14.92144650228379\n",
      "SGD iter. 18/49: loss=51.92779029470119, w0=64.84355743060398, w1=14.773985767968114\n",
      "SGD iter. 19/49: loss=46.80480980609744, w0=65.6416655232683, w1=15.548725401752405\n",
      "SGD iter. 20/49: loss=45.57755001879135, w0=66.02406613882731, w1=16.224256117842003\n",
      "SGD iter. 21/49: loss=35.07102476727577, w0=67.80446372027113, w1=16.518811243665095\n",
      "SGD iter. 22/49: loss=25.12532533202796, w0=69.0971782374208, w1=14.845807856246235\n",
      "SGD iter. 23/49: loss=22.015199242713926, w0=69.87165598907386, w1=14.723383647094431\n",
      "SGD iter. 24/49: loss=23.85196803422902, w0=69.40039457576417, w1=14.81110437995848\n",
      "SGD iter. 25/49: loss=20.966510394346436, w0=70.22507597450817, w1=14.800102191816963\n",
      "SGD iter. 26/49: loss=20.793642399784797, w0=70.4371345001856, w1=15.108906785208461\n",
      "SGD iter. 27/49: loss=18.457493196318563, w0=71.01848779820176, w1=14.46236692090192\n",
      "SGD iter. 28/49: loss=23.177601519595513, w0=69.89919395702705, w1=15.494470187546954\n",
      "SGD iter. 29/49: loss=23.587039648210336, w0=69.46328396718678, w1=14.794442751136929\n",
      "SGD iter. 30/49: loss=18.85332055507238, w0=70.80440968829684, w1=12.621112521244246\n",
      "SGD iter. 31/49: loss=18.78993307944164, w0=70.84070718991626, w1=12.590990033740319\n",
      "SGD iter. 32/49: loss=18.387251301312904, w0=70.97792348252953, w1=12.680414166324352\n",
      "SGD iter. 33/49: loss=18.1349435565138, w0=72.47349227581152, w1=11.28312270552601\n",
      "SGD iter. 34/49: loss=18.374606981598713, w0=71.63251092282093, w1=11.68607048321324\n",
      "SGD iter. 35/49: loss=18.851514877512695, w0=71.99320537637728, w1=11.190740989793284\n",
      "SGD iter. 36/49: loss=18.191447150164407, w0=72.30467741094277, w1=11.327384927027975\n",
      "SGD iter. 37/49: loss=18.538787399774563, w0=72.22188065182586, w1=11.20891380781367\n",
      "SGD iter. 38/49: loss=17.484035804835038, w0=72.55523048609405, w1=11.56905005869531\n",
      "SGD iter. 39/49: loss=17.73477969485579, w0=72.43095172987432, w1=11.491480566896556\n",
      "SGD iter. 40/49: loss=17.812326288762698, w0=72.56523287590143, w1=11.400797093466217\n",
      "SGD iter. 41/49: loss=17.604895593682944, w0=71.72618703637893, w1=12.07250863775626\n",
      "SGD iter. 42/49: loss=16.082653832803665, w0=72.19437704593906, w1=13.050139603773937\n",
      "SGD iter. 43/49: loss=15.963895041422331, w0=72.36035222705524, w1=12.946362812040782\n",
      "SGD iter. 44/49: loss=18.41351579439432, w0=71.24423402051671, w1=12.118082885786475\n",
      "SGD iter. 45/49: loss=17.48499732139968, w0=71.67035249537027, w1=12.229816054507417\n",
      "SGD iter. 46/49: loss=16.709969253106458, w0=72.30972637808586, w1=12.183748800385318\n",
      "SGD iter. 47/49: loss=16.494886402881068, w0=72.47570505395139, w1=12.23531776533749\n",
      "SGD iter. 48/49: loss=16.52324120451989, w0=72.49518346795615, w1=12.200367514552644\n",
      "SGD iter. 49/49: loss=15.458603111273327, w0=72.91593631114192, w1=13.53028221168298\n",
      "SGD: execution time=2.526 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5affddb1f331427a9014c6562e844b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "# ***************************************************\n",
    "# raise NotImplementedError\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.847464098448484, w1=7.724426406192441\n",
      "GD iter. 1/49: loss=318.282124701595, w0=67.401703327983, w1=10.041754328050121\n",
      "GD iter. 2/49: loss=88.6423556165126, w0=72.06797509684336, w1=10.736952704607413\n",
      "GD iter. 3/49: loss=67.97477639885521, w0=73.46785662750146, w1=10.945512217574594\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631796\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.05160722578589, w1=11.03248153448191\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=65.93074217249236, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889338, w0=74.06736849193958, w1=11.034829706038407\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003893\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225756, w1=11.034889001593537\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670431\n",
      "GD iter. 13/49: loss=65.93073010267466, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608876, w1=11.034894818487498\n",
      "GD iter. 16/49: loss=65.93073010260342, w0=74.06780575927509, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260338, w0=74.06780582623098, w1=11.034894861713957\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706558\n",
      "GD iter. 19/49: loss=65.93073010260339, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260338, w0=74.06780585415159, w1=11.034894865873671\n",
      "GD iter. 21/49: loss=65.93073010260338, w0=74.06780585469393, w1=11.03489486595447\n",
      "GD iter. 22/49: loss=65.93073010260336, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260336, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.0678058549201, w1=11.034894865988164\n",
      "GD iter. 25/49: loss=65.93073010260338, w0=74.06780585492449, w1=11.034894865988818\n",
      "GD iter. 26/49: loss=65.93073010260338, w0=74.06780585492581, w1=11.034894865989017\n",
      "GD iter. 27/49: loss=65.93073010260336, w0=74.06780585492619, w1=11.034894865989077\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.06780585492632, w1=11.034894865989093\n",
      "GD iter. 29/49: loss=65.93073010260338, w0=74.06780585492635, w1=11.034894865989099\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.034894865989102\n",
      "GD iter. 31/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260339, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.010 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "#       and the model fit\n",
    "\n",
    "# batch_size = 5\n",
    "# batch_losses, batch_weights = stochastic_gradient_descent(\n",
    "#     y, tx, w_initial, batch_size, max_iters, gamma\n",
    "# )\n",
    "\n",
    "batch_losses, batch_weights = gradient_descent(\n",
    "    y, tx, w_initial, max_iters, gamma\n",
    ")\n",
    "\n",
    "\n",
    "# ***************************************************\n",
    "# raise NotImplementedError\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ead2913b3b4e64a13ef25c54e35d48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    N = y.shape[0]\n",
    "    \n",
    "    error = y - tx @ w\n",
    "    return - tx.T @ np.sign(error) / N\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        subgradient = compute_subgradient_mae(y, tx, w)\n",
    "        error = y - tx @ w\n",
    "        loss = calculate_mae(error)\n",
    "        # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        \n",
    "        w = w - gamma * subgradient\n",
    "        # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=8.756471895211877e-16\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.7512943790423754e-15\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=2.626941568563563e-15\n",
      "SubGD iter. 3/499: loss=71.96780585492637, w0=2.8, w1=3.502588758084751e-15\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=4.378235947605939e-15\n",
      "SubGD iter. 5/499: loss=70.56780585492638, w0=4.2, w1=5.253883137127127e-15\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=6.1295303266483146e-15\n",
      "SubGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=7.0051775161695025e-15\n",
      "SubGD iter. 8/499: loss=68.46780585492638, w0=6.300000000000001, w1=7.88082470569069e-15\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=8.756471895211878e-15\n",
      "SubGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000001, w1=9.632119084733065e-15\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=1.0507766274254253e-14\n",
      "SubGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=1.1383413463775441e-14\n",
      "SubGD iter. 13/499: loss=64.96780585492637, w0=9.799999999999999, w1=1.2259060653296629e-14\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=1.3134707842817817e-14\n",
      "SubGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=1.4010355032339005e-14\n",
      "SubGD iter. 16/499: loss=62.86780585492639, w0=11.899999999999997, w1=1.488600222186019e-14\n",
      "SubGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.576164941138138e-14\n",
      "SubGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.6637296600902567e-14\n",
      "SubGD iter. 19/499: loss=60.767805854926394, w0=13.999999999999995, w1=1.7512943790423755e-14\n",
      "SubGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.8388590979944943e-14\n",
      "SubGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.926423816946613e-14\n",
      "SubGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=2.013988535898732e-14\n",
      "SubGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=2.1015532548508507e-14\n",
      "SubGD iter. 24/499: loss=57.267805854926394, w0=17.499999999999993, w1=2.1891179738029695e-14\n",
      "SubGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=2.2766826927550882e-14\n",
      "SubGD iter. 26/499: loss=55.86780585492639, w0=18.89999999999999, w1=2.364247411707207e-14\n",
      "SubGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=2.4518121306593258e-14\n",
      "SubGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=2.5393768496114446e-14\n",
      "SubGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=2.6269415685635634e-14\n",
      "SubGD iter. 30/499: loss=53.067805854926384, w0=21.69999999999999, w1=2.7145062875156822e-14\n",
      "SubGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=2.802071006467801e-14\n",
      "SubGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.8896357254199195e-14\n",
      "SubGD iter. 33/499: loss=50.96780585492638, w0=23.799999999999986, w1=2.977200444372038e-14\n",
      "SubGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=3.064765163324157e-14\n",
      "SubGD iter. 35/499: loss=49.567805854926405, w0=25.199999999999985, w1=3.152329882276276e-14\n",
      "SubGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=3.2398946012283946e-14\n",
      "SubGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=3.3274593201805134e-14\n",
      "SubGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=3.415024039132632e-14\n",
      "SubGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=3.502588758084751e-14\n",
      "SubGD iter. 40/499: loss=46.06780585492639, w0=28.69999999999998, w1=3.59015347703687e-14\n",
      "SubGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=3.6777181959889886e-14\n",
      "SubGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=3.7652829149411074e-14\n",
      "SubGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=3.852847633893226e-14\n",
      "SubGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=3.940412352845345e-14\n",
      "SubGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=4.027977071797464e-14\n",
      "SubGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=4.1155417907495825e-14\n",
      "SubGD iter. 47/499: loss=41.1678058549264, w0=33.59999999999999, w1=4.2031065097017013e-14\n",
      "SubGD iter. 48/499: loss=40.4678058549264, w0=34.29999999999999, w1=4.29067122865382e-14\n",
      "SubGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=4.378235947605939e-14\n",
      "SubGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=4.465800666558058e-14\n",
      "SubGD iter. 51/499: loss=38.36780585492639, w0=36.4, w1=4.5533653855101765e-14\n",
      "SubGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=4.640930104462295e-14\n",
      "SubGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=4.728494823414414e-14\n",
      "SubGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=4.816059542366533e-14\n",
      "SubGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=4.9036242613186517e-14\n",
      "SubGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=4.9911889802707705e-14\n",
      "SubGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=5.078753699222889e-14\n",
      "SubGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=5.166318418175008e-14\n",
      "SubGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=5.253883137127127e-14\n",
      "SubGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=5.3414478560792456e-14\n",
      "SubGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=5.4290125750313644e-14\n",
      "SubGD iter. 62/499: loss=30.667805854926346, w0=44.10000000000003, w1=5.516577293983483e-14\n",
      "SubGD iter. 63/499: loss=29.967805854926347, w0=44.80000000000003, w1=5.604142012935602e-14\n",
      "SubGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=5.691706731887721e-14\n",
      "SubGD iter. 65/499: loss=28.567805854926338, w0=46.20000000000004, w1=5.779271450839839e-14\n",
      "SubGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=5.866836169791957e-14\n",
      "SubGD iter. 67/499: loss=27.173270209668917, w0=47.59306930693074, w1=0.01114784567828894\n",
      "SubGD iter. 68/499: loss=26.4904515637512, w0=48.279207920792125, w1=0.03308574108991741\n",
      "SubGD iter. 69/499: loss=25.81721232277017, w0=48.96534653465351, w1=0.055023636501545875\n",
      "SubGD iter. 70/499: loss=25.15503943465645, w0=49.63069306930698, w1=0.1053832638830964\n",
      "SubGD iter. 71/499: loss=24.524103413894778, w0=50.28910891089114, w1=0.16746568532795278\n",
      "SubGD iter. 72/499: loss=23.899295346035586, w0=50.947524752475296, w1=0.22954810677280915\n",
      "SubGD iter. 73/499: loss=23.28439292565714, w0=51.59207920792084, w1=0.312425129327494\n",
      "SubGD iter. 74/499: loss=22.68687644418184, w0=52.22277227722777, w1=0.41195013288401805\n",
      "SubGD iter. 75/499: loss=22.10626756964055, w0=52.84653465346539, w1=0.5208167847923948\n",
      "SubGD iter. 76/499: loss=21.53781882800843, w0=53.4564356435644, w1=0.6457900912636185\n",
      "SubGD iter. 77/499: loss=20.986339874628463, w0=54.0594059405941, w1=0.7796904498577408\n",
      "SubGD iter. 78/499: loss=20.445560936620446, w0=54.655445544554496, w1=0.9197570104995888\n",
      "SubGD iter. 79/499: loss=19.91191015895784, w0=55.24455445544559, w1=1.067092029785011\n",
      "SubGD iter. 80/499: loss=19.389644090563227, w0=55.819801980198065, w1=1.2261255948210965\n",
      "SubGD iter. 81/499: loss=18.887989064395878, w0=56.36732673267331, w1=1.410709342622233\n",
      "SubGD iter. 82/499: loss=18.41596050185423, w0=56.900990099009945, w1=1.605853732220289\n",
      "SubGD iter. 83/499: loss=17.954898543040382, w0=57.42772277227727, w1=1.808762802293982\n",
      "SubGD iter. 84/499: loss=17.505757656579817, w0=57.933663366336674, w1=2.0285064197514897\n",
      "SubGD iter. 85/499: loss=17.074957426931608, w0=58.43267326732677, w1=2.2494370848672975\n",
      "SubGD iter. 86/499: loss=16.652967297509893, w0=58.91089108910895, w1=2.4837982986028537\n",
      "SubGD iter. 87/499: loss=16.248540731496718, w0=59.382178217821824, w1=2.7260245553531703\n",
      "SubGD iter. 88/499: loss=15.849105212654152, w0=59.83960396039608, w1=2.978742333469156\n",
      "SubGD iter. 89/499: loss=15.466919791231321, w0=60.262376237623805, w1=3.251528669355458\n",
      "SubGD iter. 90/499: loss=15.108294621512211, w0=60.67821782178222, w1=3.5270865794243\n",
      "SubGD iter. 91/499: loss=14.754896345922827, w0=61.087128712871326, w1=3.806459183951836\n",
      "SubGD iter. 92/499: loss=14.404528961620272, w0=61.49603960396043, w1=4.085831788479371\n",
      "SubGD iter. 93/499: loss=14.05578702812727, w0=61.891089108910926, w1=4.373839384328629\n",
      "SubGD iter. 94/499: loss=13.714620911605627, w0=62.27920792079211, w1=4.666037469532069\n",
      "SubGD iter. 95/499: loss=13.381236307284146, w0=62.65346534653469, w1=4.959829093241791\n",
      "SubGD iter. 96/499: loss=13.058821615166227, w0=63.02079207920796, w1=5.257057192056662\n",
      "SubGD iter. 97/499: loss=12.740251724339231, w0=63.38118811881192, w1=5.5604343163524295\n",
      "SubGD iter. 98/499: loss=12.423218888756102, w0=63.74158415841588, w1=5.863811440648197\n",
      "SubGD iter. 99/499: loss=12.107561731901159, w0=64.08811881188123, w1=6.172402175278572\n",
      "SubGD iter. 100/499: loss=11.800622097398126, w0=64.42772277227726, w1=6.4863693105165225\n",
      "SubGD iter. 101/499: loss=11.495041794646415, w0=64.7673267326733, w1=6.800336445754473\n",
      "SubGD iter. 102/499: loss=11.189461491894704, w0=65.10693069306933, w1=7.1143035809924235\n",
      "SubGD iter. 103/499: loss=10.883881189142992, w0=65.44653465346536, w1=7.428270716230374\n",
      "SubGD iter. 104/499: loss=10.58459340831319, w0=65.76534653465349, w1=7.747893210218651\n",
      "SubGD iter. 105/499: loss=10.295816534318933, w0=66.070297029703, w1=8.073669686866932\n",
      "SubGD iter. 106/499: loss=10.01135208122135, w0=66.37524752475251, w1=8.399446163515213\n",
      "SubGD iter. 107/499: loss=9.728084326668117, w0=66.6663366336634, w1=8.73297028041742\n",
      "SubGD iter. 108/499: loss=9.448125461122496, w0=66.9574257425743, w1=9.066494397319628\n",
      "SubGD iter. 109/499: loss=9.171041104096656, w0=67.23465346534658, w1=9.398630319470323\n",
      "SubGD iter. 110/499: loss=8.903656131158947, w0=67.51188118811886, w1=9.730766241621017\n",
      "SubGD iter. 111/499: loss=8.63627115822124, w0=67.78910891089114, w1=10.062902163771712\n",
      "SubGD iter. 112/499: loss=8.376151920302359, w0=68.06633663366343, w1=10.363999289979459\n",
      "SubGD iter. 113/499: loss=8.140540838751482, w0=68.32970297029709, w1=10.66046690927365\n",
      "SubGD iter. 114/499: loss=7.918544501597259, w0=68.59306930693076, w1=10.943174379960851\n",
      "SubGD iter. 115/499: loss=7.7052797283769845, w0=68.85643564356442, w1=11.225881850648053\n",
      "SubGD iter. 116/499: loss=7.493695831178626, w0=69.11287128712878, w1=11.504395843582245\n",
      "SubGD iter. 117/499: loss=7.2899924057434, w0=69.35544554455453, w1=11.78820189306779\n",
      "SubGD iter. 118/499: loss=7.097234035781528, w0=69.58415841584166, w1=12.06091146519101\n",
      "SubGD iter. 119/499: loss=6.919905294668907, w0=69.80594059405948, w1=12.324245668386087\n",
      "SubGD iter. 120/499: loss=6.7505735273154395, w0=70.0277227722773, w1=12.587579871581164\n",
      "SubGD iter. 121/499: loss=6.584744810805652, w0=70.25643564356443, w1=12.824765405096523\n",
      "SubGD iter. 122/499: loss=6.4303432763477915, w0=70.47821782178225, w1=13.065616959310187\n",
      "SubGD iter. 123/499: loss=6.27807148189034, w0=70.69306930693077, w1=13.302953389983951\n",
      "SubGD iter. 124/499: loss=6.133663329263311, w0=70.89405940594067, w1=13.525403099312957\n",
      "SubGD iter. 125/499: loss=6.005840798343018, w0=71.08811881188126, w1=13.742945617944251\n",
      "SubGD iter. 126/499: loss=5.885021825223206, w0=71.27524752475254, w1=13.953548196006883\n",
      "SubGD iter. 127/499: loss=5.771635252269647, w0=71.46237623762383, w1=14.164150774069515\n",
      "SubGD iter. 128/499: loss=5.667162061790248, w0=71.62178217821788, w1=14.349779559473214\n",
      "SubGD iter. 129/499: loss=5.586726765993136, w0=71.75346534653471, w1=14.516890107612351\n",
      "SubGD iter. 130/499: loss=5.523847812160378, w0=71.87128712871292, w1=14.670791185324227\n",
      "SubGD iter. 131/499: loss=5.480093708591866, w0=71.95445544554461, w1=14.780276456654562\n",
      "SubGD iter. 132/499: loss=5.453088003502018, w0=72.0376237623763, w1=14.889761727984897\n",
      "SubGD iter. 133/499: loss=5.4273926308629, w0=72.10693069306937, w1=14.985916181776767\n",
      "SubGD iter. 134/499: loss=5.407322445682747, w0=72.17623762376245, w1=15.082070635568638\n",
      "SubGD iter. 135/499: loss=5.387252260502595, w0=72.24554455445552, w1=15.178225089360508\n",
      "SubGD iter. 136/499: loss=5.370460780338691, w0=72.30099009900998, w1=15.25972348971595\n",
      "SubGD iter. 137/499: loss=5.3574065233347365, w0=72.34950495049513, w1=15.335091856448178\n",
      "SubGD iter. 138/499: loss=5.345929264022579, w0=72.39801980198028, w1=15.410460223180406\n",
      "SubGD iter. 139/499: loss=5.335714659517469, w0=72.43267326732682, w1=15.469961786755766\n",
      "SubGD iter. 140/499: loss=5.330043910465359, w0=72.46039603960405, w1=15.51864528583285\n",
      "SubGD iter. 141/499: loss=5.325676428273224, w0=72.48811881188128, w1=15.561592159086528\n",
      "SubGD iter. 142/499: loss=5.322176726526588, w0=72.5019801980199, w1=15.597828332032567\n",
      "SubGD iter. 143/499: loss=5.32011130964311, w0=72.52277227722782, w1=15.624722856626754\n",
      "SubGD iter. 144/499: loss=5.318478284898438, w0=72.55049504950505, w1=15.642690329098041\n",
      "SubGD iter. 145/499: loss=5.317240048565146, w0=72.56435643564366, w1=15.664356578291132\n",
      "SubGD iter. 146/499: loss=5.316406547951545, w0=72.58514851485158, w1=15.677095775361325\n",
      "SubGD iter. 147/499: loss=5.315557122666141, w0=72.6059405940595, w1=15.689834972431518\n",
      "SubGD iter. 148/499: loss=5.31470769738074, w0=72.62673267326743, w1=15.70257416950171\n",
      "SubGD iter. 149/499: loss=5.313876880922164, w0=72.64059405940604, w1=15.724240418694801\n",
      "SubGD iter. 150/499: loss=5.313052246871382, w0=72.66138613861396, w1=15.736979615764994\n",
      "SubGD iter. 151/499: loss=5.3123778390243865, w0=72.66831683168327, w1=15.74811029423132\n",
      "SubGD iter. 152/499: loss=5.312132229725042, w0=72.67524752475258, w1=15.759240972697647\n",
      "SubGD iter. 153/499: loss=5.311886620425695, w0=72.68217821782189, w1=15.770371651163973\n",
      "SubGD iter. 154/499: loss=5.311683566098434, w0=72.68217821782189, w1=15.774323911906727\n",
      "SubGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.77827617264948\n",
      "SubGD iter. 156/499: loss=5.311638936484209, w0=72.68217821782189, w1=15.782228433392234\n",
      "SubGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.786180694134988\n",
      "SubGD iter. 158/499: loss=5.311594306869984, w0=72.68217821782189, w1=15.790132954877741\n",
      "SubGD iter. 159/499: loss=5.311571992062872, w0=72.68217821782189, w1=15.794085215620495\n",
      "SubGD iter. 160/499: loss=5.311549677255758, w0=72.68217821782189, w1=15.798037476363248\n",
      "SubGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.801989737106002\n",
      "SubGD iter. 162/499: loss=5.311505047641535, w0=72.68217821782189, w1=15.805941997848755\n",
      "SubGD iter. 163/499: loss=5.3114827328344205, w0=72.68217821782189, w1=15.809894258591509\n",
      "SubGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.813846519334263\n",
      "SubGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.817798780077016\n",
      "SubGD iter. 166/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.82175104081977\n",
      "SubGD iter. 167/499: loss=5.311393473605971, w0=72.68217821782189, w1=15.825703301562523\n",
      "SubGD iter. 168/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.829655562305277\n",
      "SubGD iter. 169/499: loss=5.311348843991747, w0=72.68217821782189, w1=15.83360782304803\n",
      "SubGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.837560083790784\n",
      "SubGD iter. 171/499: loss=5.311304214377522, w0=72.68217821782189, w1=15.841512344533538\n",
      "SubGD iter. 172/499: loss=5.31128189957041, w0=72.68217821782189, w1=15.845464605276291\n",
      "SubGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.849416866019045\n",
      "SubGD iter. 174/499: loss=5.311237269956185, w0=72.68217821782189, w1=15.853369126761798\n",
      "SubGD iter. 175/499: loss=5.311214955149072, w0=72.68217821782189, w1=15.857321387504552\n",
      "SubGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.861273648247305\n",
      "SubGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.865225908990059\n",
      "SubGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.869178169732812\n",
      "SubGD iter. 179/499: loss=5.311125695920622, w0=72.68217821782189, w1=15.873130430475566\n",
      "SubGD iter. 180/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.87708269121832\n",
      "SubGD iter. 181/499: loss=5.311081066306398, w0=72.68217821782189, w1=15.881034951961073\n",
      "SubGD iter. 182/499: loss=5.311058751499286, w0=72.68217821782189, w1=15.884987212703827\n",
      "SubGD iter. 183/499: loss=5.311036436692172, w0=72.68217821782189, w1=15.88893947344658\n",
      "SubGD iter. 184/499: loss=5.31101412188506, w0=72.68217821782189, w1=15.892891734189334\n",
      "SubGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.896843994932087\n",
      "SubGD iter. 186/499: loss=5.310969492270836, w0=72.68217821782189, w1=15.900796255674841\n",
      "SubGD iter. 187/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.904748516417595\n",
      "SubGD iter. 188/499: loss=5.310924862656612, w0=72.68217821782189, w1=15.908700777160348\n",
      "SubGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.912653037903102\n",
      "SubGD iter. 190/499: loss=5.310913706061381, w0=72.67524752475258, w1=15.910526938117364\n",
      "SubGD iter. 191/499: loss=5.310892237186269, w0=72.67524752475258, w1=15.914479198860118\n",
      "SubGD iter. 192/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.918431459602871\n",
      "SubGD iter. 193/499: loss=5.310862636053835, w0=72.66831683168327, w1=15.916305359817134\n",
      "SubGD iter. 194/499: loss=5.310859611715928, w0=72.66831683168327, w1=15.920257620559887\n",
      "SubGD iter. 195/499: loss=5.310837296908815, w0=72.66831683168327, w1=15.924209881302641\n",
      "SubGD iter. 196/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.928162142045394\n",
      "SubGD iter. 197/499: loss=5.310823570190172, w0=72.66138613861396, w1=15.926036042259657\n",
      "SubGD iter. 198/499: loss=5.310804671438475, w0=72.66138613861396, w1=15.92998830300241\n",
      "SubGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.933940563745164\n",
      "SubGD iter. 200/499: loss=5.310772500182623, w0=72.65445544554466, w1=15.931814463959427\n",
      "SubGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.93576672470218\n",
      "SubGD iter. 202/499: loss=5.310749731161019, w0=72.65445544554466, w1=15.939718985444934\n",
      "SubGD iter. 203/499: loss=5.310727416353907, w0=72.65445544554466, w1=15.943671246187687\n",
      "SubGD iter. 204/499: loss=5.310733434318959, w0=72.64752475247535, w1=15.94154514640195\n",
      "SubGD iter. 205/499: loss=5.310717105690678, w0=72.64752475247535, w1=15.945497407144703\n",
      "SubGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.949449667887457\n",
      "SubGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940604, w1=15.94732356810172\n",
      "SubGD iter. 208/499: loss=5.310684480220336, w0=72.64059405940604, w1=15.951275828844473\n",
      "SubGD iter. 209/499: loss=5.310662165413224, w0=72.64059405940604, w1=15.955228089587226\n",
      "SubGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.95918035032998\n",
      "SubGD iter. 211/499: loss=5.310643298447746, w0=72.63366336633673, w1=15.957054250544243\n",
      "SubGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.961006511286996\n",
      "SubGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.96495877202975\n",
      "SubGD iter. 214/499: loss=5.3105922284402, w0=72.62673267326743, w1=15.962832672244012\n",
      "SubGD iter. 215/499: loss=5.310633183099026, w0=72.63366336633673, w1=15.967301372051416\n",
      "SubGD iter. 216/499: loss=5.3105993435850625, w0=72.62673267326743, w1=15.965175272265679\n",
      "SubGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633673, w1=15.969643972073083\n",
      "SubGD iter. 218/499: loss=5.310606458729926, w0=72.62673267326743, w1=15.967517872287345\n",
      "SubGD iter. 219/499: loss=5.3106032734525535, w0=72.63366336633673, w1=15.97198657209475\n",
      "SubGD iter. 220/499: loss=5.3106135738747895, w0=72.62673267326743, w1=15.969860472309012\n",
      "SubGD iter. 221/499: loss=5.310588318629316, w0=72.63366336633673, w1=15.974329172116416\n",
      "SubGD iter. 222/499: loss=5.310620689019653, w0=72.62673267326743, w1=15.972203072330679\n",
      "SubGD iter. 223/499: loss=5.310574966149885, w0=72.62673267326743, w1=15.970593411609592\n",
      "SubGD iter. 224/499: loss=5.31058363964973, w0=72.63366336633673, w1=15.975062111416996\n",
      "SubGD iter. 225/499: loss=5.310622915165494, w0=72.62673267326743, w1=15.972936011631258\n",
      "SubGD iter. 226/499: loss=5.310576651555034, w0=72.62673267326743, w1=15.971326350910171\n",
      "SubGD iter. 227/499: loss=5.310578960670142, w0=72.63366336633673, w1=15.975795050717576\n",
      "SubGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326743, w1=15.973668950931838\n",
      "SubGD iter. 229/499: loss=5.31057833696018, w0=72.62673267326743, w1=15.972059290210751\n",
      "SubGD iter. 230/499: loss=5.310574635520698, w0=72.62673267326743, w1=15.970449629489664\n",
      "SubGD iter. 231/499: loss=5.310584557534202, w0=72.63366336633673, w1=15.974918329297068\n",
      "SubGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326743, w1=15.972792229511331\n",
      "SubGD iter. 233/499: loss=5.310576320925845, w0=72.62673267326743, w1=15.971182568790244\n",
      "SubGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633673, w1=15.975651268597648\n",
      "SubGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326743, w1=15.97352516881191\n",
      "SubGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.971915508090824\n",
      "SubGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633673, w1=15.976384207898228\n",
      "SubGD iter. 238/499: loss=5.310626930749844, w0=72.62673267326743, w1=15.97425810811249\n",
      "SubGD iter. 239/499: loss=5.310579691736141, w0=72.62673267326743, w1=15.972648447391403\n",
      "SubGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.971038786670317\n",
      "SubGD iter. 241/499: loss=5.310580796439089, w0=72.63366336633673, w1=15.97550748647772\n",
      "SubGD iter. 242/499: loss=5.310624267896666, w0=72.62673267326743, w1=15.973381386691983\n",
      "SubGD iter. 243/499: loss=5.310577675701806, w0=72.62673267326743, w1=15.971771725970896\n",
      "SubGD iter. 244/499: loss=5.3105761174595, w0=72.63366336633673, w1=15.9762404257783\n",
      "SubGD iter. 245/499: loss=5.310626494042512, w0=72.62673267326743, w1=15.974114325992563\n",
      "SubGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.972504665271476\n",
      "SubGD iter. 247/499: loss=5.310575659667473, w0=72.62673267326743, w1=15.970895004550389\n",
      "SubGD iter. 248/499: loss=5.310581714323562, w0=72.63366336633673, w1=15.975363704357793\n",
      "SubGD iter. 249/499: loss=5.310623831189333, w0=72.62673267326743, w1=15.973237604572056\n",
      "SubGD iter. 250/499: loss=5.310577345072619, w0=72.62673267326743, w1=15.971627943850969\n",
      "SubGD iter. 251/499: loss=5.310577035343974, w0=72.63366336633673, w1=15.976096643658373\n",
      "SubGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326743, w1=15.973970543872635\n",
      "SubGD iter. 253/499: loss=5.310579030477768, w0=72.62673267326743, w1=15.972360883151548\n",
      "SubGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.970751222430462\n",
      "SubGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633673, w1=15.975219922237866\n",
      "SubGD iter. 256/499: loss=5.310623394482, w0=72.62673267326743, w1=15.973093822452128\n",
      "SubGD iter. 257/499: loss=5.310577014443433, w0=72.62673267326743, w1=15.971484161731041\n",
      "SubGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633673, w1=15.975952861538445\n",
      "SubGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326743, w1=15.973826761752708\n",
      "SubGD iter. 260/499: loss=5.310578699848579, w0=72.62673267326743, w1=15.972217101031621\n",
      "SubGD iter. 261/499: loss=5.310574998409098, w0=72.62673267326743, w1=15.970607440310534\n",
      "SubGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633673, w1=15.975076140117938\n",
      "SubGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326743, w1=15.9729500403322\n",
      "SubGD iter. 264/499: loss=5.310576683814246, w0=72.62673267326743, w1=15.971340379611114\n",
      "SubGD iter. 265/499: loss=5.310578871112923, w0=72.63366336633673, w1=15.975809079418518\n",
      "SubGD iter. 266/499: loss=5.310625183920507, w0=72.62673267326743, w1=15.97368297963278\n",
      "SubGD iter. 267/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.972073318911693\n",
      "SubGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.970463658190607\n",
      "SubGD iter. 269/499: loss=5.310584467976983, w0=72.63366336633673, w1=15.97493235799801\n",
      "SubGD iter. 270/499: loss=5.310622521067329, w0=72.62673267326743, w1=15.972806258212273\n",
      "SubGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.971196597491186\n",
      "SubGD iter. 272/499: loss=5.310579788997396, w0=72.63366336633673, w1=15.97566529729859\n",
      "SubGD iter. 273/499: loss=5.310624747213171, w0=72.62673267326743, w1=15.973539197512853\n",
      "SubGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.971929536791766\n",
      "SubGD iter. 275/499: loss=5.310575110017808, w0=72.63366336633673, w1=15.97639823659917\n",
      "SubGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326743, w1=15.974272136813433\n",
      "SubGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.972662476092346\n",
      "SubGD iter. 278/499: loss=5.310576022555872, w0=72.62673267326743, w1=15.971052815371259\n",
      "SubGD iter. 279/499: loss=5.31058070688187, w0=72.63366336633673, w1=15.975521515178663\n",
      "SubGD iter. 280/499: loss=5.310624310505836, w0=72.62673267326743, w1=15.973395415392925\n",
      "SubGD iter. 281/499: loss=5.310577707961019, w0=72.62673267326743, w1=15.971785754671838\n",
      "SubGD iter. 282/499: loss=5.3105760279022824, w0=72.63366336633673, w1=15.976254454479243\n",
      "SubGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326743, w1=15.974128354693505\n",
      "SubGD iter. 284/499: loss=5.310579393366167, w0=72.62673267326743, w1=15.972518693972418\n",
      "SubGD iter. 285/499: loss=5.310575691926685, w0=72.62673267326743, w1=15.970909033251331\n",
      "SubGD iter. 286/499: loss=5.3105816247663435, w0=72.63366336633673, w1=15.975377733058735\n",
      "SubGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326743, w1=15.973251633272998\n",
      "SubGD iter. 288/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.971641972551911\n",
      "SubGD iter. 289/499: loss=5.310576945786756, w0=72.63366336633673, w1=15.976110672359315\n",
      "SubGD iter. 290/499: loss=5.310626099944345, w0=72.62673267326743, w1=15.973984572573578\n",
      "SubGD iter. 291/499: loss=5.310579062736981, w0=72.62673267326743, w1=15.97237491185249\n",
      "SubGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.970765251131404\n",
      "SubGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633673, w1=15.975233950938808\n",
      "SubGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326743, w1=15.97310785115307\n",
      "SubGD iter. 295/499: loss=5.310577046702646, w0=72.62673267326743, w1=15.971498190431983\n",
      "SubGD iter. 296/499: loss=5.31057786367123, w0=72.63366336633673, w1=15.975966890239388\n",
      "SubGD iter. 297/499: loss=5.310625663237009, w0=72.62673267326743, w1=15.97384079045365\n",
      "SubGD iter. 298/499: loss=5.310578732107793, w0=72.62673267326743, w1=15.972231129732563\n",
      "SubGD iter. 299/499: loss=5.310575030668311, w0=72.62673267326743, w1=15.970621469011476\n",
      "SubGD iter. 300/499: loss=5.31058346053529, w0=72.63366336633673, w1=15.97509016881888\n",
      "SubGD iter. 301/499: loss=5.310623000383833, w0=72.62673267326743, w1=15.972964069033143\n",
      "SubGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.971354408312056\n",
      "SubGD iter. 303/499: loss=5.310578781555704, w0=72.63366336633673, w1=15.97582310811946\n",
      "SubGD iter. 304/499: loss=5.310625226529675, w0=72.62673267326743, w1=15.973697008333723\n",
      "SubGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.972087347612636\n",
      "SubGD iter. 306/499: loss=5.310574700039124, w0=72.62673267326743, w1=15.970477686891549\n",
      "SubGD iter. 307/499: loss=5.310584378419764, w0=72.63366336633673, w1=15.974946386698953\n",
      "SubGD iter. 308/499: loss=5.310622563676497, w0=72.62673267326743, w1=15.972820286913215\n",
      "SubGD iter. 309/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.971210626192129\n",
      "SubGD iter. 310/499: loss=5.310579699440178, w0=72.63366336633673, w1=15.975679325999533\n",
      "SubGD iter. 311/499: loss=5.310624789822341, w0=72.62673267326743, w1=15.973553226213795\n",
      "SubGD iter. 312/499: loss=5.310578070849419, w0=72.62673267326743, w1=15.971943565492708\n",
      "SubGD iter. 313/499: loss=5.310575020460591, w0=72.63366336633673, w1=15.976412265300112\n",
      "SubGD iter. 314/499: loss=5.310627015968183, w0=72.62673267326743, w1=15.974286165514375\n",
      "SubGD iter. 315/499: loss=5.310579756254565, w0=72.62673267326743, w1=15.972676504793288\n",
      "SubGD iter. 316/499: loss=5.310576054815084, w0=72.62673267326743, w1=15.971066844072201\n",
      "SubGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633673, w1=15.975535543879605\n",
      "SubGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326743, w1=15.973409444093868\n",
      "SubGD iter. 319/499: loss=5.310577740220233, w0=72.62673267326743, w1=15.97179978337278\n",
      "SubGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633673, w1=15.976268483180185\n",
      "SubGD iter. 321/499: loss=5.310626579260847, w0=72.62673267326743, w1=15.974142383394447\n",
      "SubGD iter. 322/499: loss=5.31057942562538, w0=72.62673267326743, w1=15.97253272267336\n",
      "SubGD iter. 323/499: loss=5.310575724185898, w0=72.62673267326743, w1=15.970923061952274\n",
      "SubGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633673, w1=15.975391761759678\n",
      "SubGD iter. 325/499: loss=5.310623916407669, w0=72.62673267326743, w1=15.97326566197394\n",
      "SubGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.971656001252853\n",
      "SubGD iter. 327/499: loss=5.310576856229536, w0=72.63366336633673, w1=15.976124701060257\n",
      "SubGD iter. 328/499: loss=5.310626142553513, w0=72.62673267326743, w1=15.97399860127452\n",
      "SubGD iter. 329/499: loss=5.310579094996192, w0=72.62673267326743, w1=15.972388940553433\n",
      "SubGD iter. 330/499: loss=5.31057539355671, w0=72.62673267326743, w1=15.970779279832346\n",
      "SubGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633673, w1=15.97524797963975\n",
      "SubGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326743, w1=15.973121879854013\n",
      "SubGD iter. 333/499: loss=5.310577078961858, w0=72.62673267326743, w1=15.971512219132926\n",
      "SubGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633673, w1=15.97598091894033\n",
      "SubGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326743, w1=15.973854819154592\n",
      "SubGD iter. 336/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.972245158433505\n",
      "SubGD iter. 337/499: loss=5.310575062927524, w0=72.62673267326743, w1=15.970635497712419\n",
      "SubGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633673, w1=15.975104197519823\n",
      "SubGD iter. 339/499: loss=5.310623042993, w0=72.62673267326743, w1=15.972978097734085\n",
      "SubGD iter. 340/499: loss=5.310576748332672, w0=72.62673267326743, w1=15.971368437012998\n",
      "SubGD iter. 341/499: loss=5.310578691998485, w0=72.63366336633673, w1=15.975837136820402\n",
      "SubGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326743, w1=15.973711037034665\n",
      "SubGD iter. 343/499: loss=5.310578433737819, w0=72.62673267326743, w1=15.972101376313578\n",
      "SubGD iter. 344/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.970491715592491\n",
      "SubGD iter. 345/499: loss=5.310584288862545, w0=72.63366336633673, w1=15.974960415399895\n",
      "SubGD iter. 346/499: loss=5.3106226062856665, w0=72.62673267326743, w1=15.972834315614158\n",
      "SubGD iter. 347/499: loss=5.3105764177034835, w0=72.62673267326743, w1=15.97122465489307\n",
      "SubGD iter. 348/499: loss=5.310579609882959, w0=72.63366336633673, w1=15.975693354700475\n",
      "SubGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326743, w1=15.973567254914737\n",
      "SubGD iter. 350/499: loss=5.3105781031086305, w0=72.62673267326743, w1=15.97195759419365\n",
      "SubGD iter. 351/499: loss=5.310574930903369, w0=72.63366336633673, w1=15.976426294001055\n",
      "SubGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326743, w1=15.974300194215317\n",
      "SubGD iter. 353/499: loss=5.310579788513779, w0=72.62673267326743, w1=15.97269053349423\n",
      "SubGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.971080872773143\n",
      "SubGD iter. 355/499: loss=5.310580527767431, w0=72.63366336633673, w1=15.975549572580547\n",
      "SubGD iter. 356/499: loss=5.310624395724175, w0=72.62673267326743, w1=15.97342347279481\n",
      "SubGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.971813812073723\n",
      "SubGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633673, w1=15.976282511881127\n",
      "SubGD iter. 359/499: loss=5.310626621870016, w0=72.62673267326743, w1=15.97415641209539\n",
      "SubGD iter. 360/499: loss=5.310579457884592, w0=72.62673267326743, w1=15.972546751374303\n",
      "SubGD iter. 361/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.970937090653216\n",
      "SubGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633673, w1=15.97540579046062\n",
      "SubGD iter. 363/499: loss=5.310623959016838, w0=72.62673267326743, w1=15.973279690674882\n",
      "SubGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.971670029953795\n",
      "SubGD iter. 365/499: loss=5.310576766672318, w0=72.63366336633673, w1=15.9761387297612\n",
      "SubGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326743, w1=15.974012629975462\n",
      "SubGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.972402969254375\n",
      "SubGD iter. 368/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.970793308533288\n",
      "SubGD iter. 369/499: loss=5.310582363536379, w0=72.63366336633673, w1=15.975262008340692\n",
      "SubGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326743, w1=15.973135908554955\n",
      "SubGD iter. 371/499: loss=5.310577111221071, w0=72.62673267326743, w1=15.971526247833868\n",
      "SubGD iter. 372/499: loss=5.310577684556792, w0=72.63366336633673, w1=15.975994947641272\n",
      "SubGD iter. 373/499: loss=5.310625748455347, w0=72.62673267326743, w1=15.973868847855535\n",
      "SubGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.972259187134448\n",
      "SubGD iter. 375/499: loss=5.310575095186737, w0=72.62673267326743, w1=15.97064952641336\n",
      "SubGD iter. 376/499: loss=5.310583281420854, w0=72.63366336633673, w1=15.975118226220765\n",
      "SubGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326743, w1=15.972992126435027\n",
      "SubGD iter. 378/499: loss=5.310576780591883, w0=72.62673267326743, w1=15.97138246571394\n",
      "SubGD iter. 379/499: loss=5.310578602441266, w0=72.63366336633673, w1=15.975851165521345\n",
      "SubGD iter. 380/499: loss=5.310625311748013, w0=72.62673267326743, w1=15.973725065735607\n",
      "SubGD iter. 381/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.97211540501452\n",
      "SubGD iter. 382/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.970505744293433\n",
      "SubGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633673, w1=15.974974444100837\n",
      "SubGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326743, w1=15.9728483443151\n",
      "SubGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.971238683594013\n",
      "SubGD iter. 386/499: loss=5.310579520325739, w0=72.63366336633673, w1=15.975707383401417\n",
      "SubGD iter. 387/499: loss=5.310624875040677, w0=72.62673267326743, w1=15.97358128361568\n",
      "SubGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.971971622894593\n",
      "SubGD iter. 389/499: loss=5.310574841346153, w0=72.63366336633673, w1=15.976440322701997\n",
      "SubGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326743, w1=15.97431422291626\n",
      "SubGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.972704562195172\n",
      "SubGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.971094901474086\n",
      "SubGD iter. 393/499: loss=5.310580438210212, w0=72.63366336633673, w1=15.97556360128149\n",
      "SubGD iter. 394/499: loss=5.310624438333342, w0=72.62673267326743, w1=15.973437501495752\n",
      "SubGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.971827840774665\n",
      "SubGD iter. 396/499: loss=5.310575759230625, w0=72.63366336633673, w1=15.97629654058207\n",
      "SubGD iter. 397/499: loss=5.3106266644791855, w0=72.62673267326743, w1=15.974170440796332\n",
      "SubGD iter. 398/499: loss=5.310579490143805, w0=72.62673267326743, w1=15.972560780075245\n",
      "SubGD iter. 399/499: loss=5.310575788704324, w0=72.62673267326743, w1=15.970951119354158\n",
      "SubGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633673, w1=15.975419819161562\n",
      "SubGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326743, w1=15.973293719375825\n",
      "SubGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.971684058654738\n",
      "SubGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633673, w1=15.976152758462142\n",
      "SubGD iter. 404/499: loss=5.31062622777185, w0=72.62673267326743, w1=15.974026658676404\n",
      "SubGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.972416997955317\n",
      "SubGD iter. 406/499: loss=5.310575458075137, w0=72.62673267326743, w1=15.97080733723423\n",
      "SubGD iter. 407/499: loss=5.31058227397916, w0=72.63366336633673, w1=15.975276037041635\n",
      "SubGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326743, w1=15.973149937255897\n",
      "SubGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.97154027653481\n",
      "SubGD iter. 410/499: loss=5.310577594999573, w0=72.63366336633673, w1=15.976008976342214\n",
      "SubGD iter. 411/499: loss=5.310625791064515, w0=72.62673267326743, w1=15.973882876556477\n",
      "SubGD iter. 412/499: loss=5.310578828885431, w0=72.62673267326743, w1=15.97227321583539\n",
      "SubGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.970663555114303\n",
      "SubGD iter. 414/499: loss=5.310583191863635, w0=72.63366336633673, w1=15.975132254921707\n",
      "SubGD iter. 415/499: loss=5.310623128211339, w0=72.62673267326743, w1=15.97300615513597\n",
      "SubGD iter. 416/499: loss=5.310576812851096, w0=72.62673267326743, w1=15.971396494414883\n",
      "SubGD iter. 417/499: loss=5.310578512884047, w0=72.63366336633673, w1=15.975865194222287\n",
      "SubGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326743, w1=15.97373909443655\n",
      "SubGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.972129433715462\n",
      "SubGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.970519772994376\n",
      "SubGD iter. 421/499: loss=5.310584109748109, w0=72.63366336633673, w1=15.97498847280178\n",
      "SubGD iter. 422/499: loss=5.310622691504003, w0=72.62673267326743, w1=15.972862373016042\n",
      "SubGD iter. 423/499: loss=5.31057648222191, w0=72.62673267326743, w1=15.971252712294955\n",
      "SubGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633673, w1=15.97572141210236\n",
      "SubGD iter. 425/499: loss=5.310624917649846, w0=72.62673267326743, w1=15.973595312316622\n",
      "SubGD iter. 426/499: loss=5.310578167627058, w0=72.62673267326743, w1=15.971985651595535\n",
      "SubGD iter. 427/499: loss=5.310574751788933, w0=72.63366336633673, w1=15.97645435140294\n",
      "SubGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326743, w1=15.974328251617202\n",
      "SubGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.972718590896115\n",
      "SubGD iter. 430/499: loss=5.3105761515927234, w0=72.62673267326743, w1=15.971108930175028\n",
      "SubGD iter. 431/499: loss=5.310580348652993, w0=72.63366336633673, w1=15.975577629982432\n",
      "SubGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326743, w1=15.973451530196694\n",
      "SubGD iter. 433/499: loss=5.310577836997869, w0=72.62673267326743, w1=15.971841869475607\n",
      "SubGD iter. 434/499: loss=5.310575669673406, w0=72.63366336633673, w1=15.976310569283012\n",
      "SubGD iter. 435/499: loss=5.310626707088355, w0=72.62673267326743, w1=15.974184469497274\n",
      "SubGD iter. 436/499: loss=5.310579522403018, w0=72.62673267326743, w1=15.972574808776187\n",
      "SubGD iter. 437/499: loss=5.310575820963535, w0=72.62673267326743, w1=15.9709651480551\n",
      "SubGD iter. 438/499: loss=5.3105812665374685, w0=72.63366336633673, w1=15.975433847862504\n",
      "SubGD iter. 439/499: loss=5.310624044235176, w0=72.62673267326743, w1=15.973307748076767\n",
      "SubGD iter. 440/499: loss=5.310577506368682, w0=72.62673267326743, w1=15.97169808735568\n",
      "SubGD iter. 441/499: loss=5.310576587557881, w0=72.63366336633673, w1=15.976166787163084\n",
      "SubGD iter. 442/499: loss=5.310626270381018, w0=72.62673267326743, w1=15.974040687377347\n",
      "SubGD iter. 443/499: loss=5.310579191773829, w0=72.62673267326743, w1=15.97243102665626\n",
      "SubGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.970821365935173\n",
      "SubGD iter. 445/499: loss=5.310582184421942, w0=72.63366336633673, w1=15.975290065742577\n",
      "SubGD iter. 446/499: loss=5.310623607527842, w0=72.62673267326743, w1=15.97316396595684\n",
      "SubGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.971554305235752\n",
      "SubGD iter. 448/499: loss=5.310577505442354, w0=72.63366336633673, w1=15.976023005043157\n",
      "SubGD iter. 449/499: loss=5.310625833673685, w0=72.62673267326743, w1=15.97389690525742\n",
      "SubGD iter. 450/499: loss=5.310578861144642, w0=72.62673267326743, w1=15.972287244536332\n",
      "SubGD iter. 451/499: loss=5.310575159705164, w0=72.62673267326743, w1=15.970677583815245\n",
      "SubGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633673, w1=15.97514628362265\n",
      "SubGD iter. 453/499: loss=5.310623170820506, w0=72.62673267326743, w1=15.973020183836912\n",
      "SubGD iter. 454/499: loss=5.31057684511031, w0=72.62673267326743, w1=15.971410523115825\n",
      "SubGD iter. 455/499: loss=5.310578423326829, w0=72.63366336633673, w1=15.97587922292323\n",
      "SubGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326743, w1=15.973753123137492\n",
      "SubGD iter. 457/499: loss=5.310578530515457, w0=72.62673267326743, w1=15.972143462416405\n",
      "SubGD iter. 458/499: loss=5.310574829075975, w0=72.62673267326743, w1=15.970533801695318\n",
      "SubGD iter. 459/499: loss=5.31058402019089, w0=72.63366336633673, w1=15.975002501502722\n",
      "SubGD iter. 460/499: loss=5.3106227341131715, w0=72.62673267326743, w1=15.972876401716984\n",
      "SubGD iter. 461/499: loss=5.310576514481121, w0=72.62673267326743, w1=15.971266740995897\n",
      "SubGD iter. 462/499: loss=5.310579341211301, w0=72.63366336633673, w1=15.975735440803302\n",
      "SubGD iter. 463/499: loss=5.310624960259015, w0=72.62673267326743, w1=15.973609341017564\n",
      "SubGD iter. 464/499: loss=5.310578199886269, w0=72.62673267326743, w1=15.971999680296477\n",
      "SubGD iter. 465/499: loss=5.310574662231715, w0=72.63366336633673, w1=15.976468380103881\n",
      "SubGD iter. 466/499: loss=5.310627186404856, w0=72.62673267326743, w1=15.974342280318144\n",
      "SubGD iter. 467/499: loss=5.310579885291418, w0=72.62673267326743, w1=15.972732619597057\n",
      "SubGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.97112295887597\n",
      "SubGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633673, w1=15.975591658683374\n",
      "SubGD iter. 470/499: loss=5.31062452355168, w0=72.62673267326743, w1=15.973465558897637\n",
      "SubGD iter. 471/499: loss=5.310577869257082, w0=72.62673267326743, w1=15.97185589817655\n",
      "SubGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633673, w1=15.976324597983954\n",
      "SubGD iter. 473/499: loss=5.310626749697523, w0=72.62673267326743, w1=15.974198498198216\n",
      "SubGD iter. 474/499: loss=5.310579554662228, w0=72.62673267326743, w1=15.97258883747713\n",
      "SubGD iter. 475/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.970979176756043\n",
      "SubGD iter. 476/499: loss=5.310581176980249, w0=72.63366336633673, w1=15.975447876563447\n",
      "SubGD iter. 477/499: loss=5.310624086844345, w0=72.62673267326743, w1=15.97332177677771\n",
      "SubGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.971712116056622\n",
      "SubGD iter. 479/499: loss=5.310576498000661, w0=72.63366336633673, w1=15.976180815864026\n",
      "SubGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326743, w1=15.974054716078289\n",
      "SubGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.972445055357202\n",
      "SubGD iter. 482/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.970835394636115\n",
      "SubGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633673, w1=15.97530409444352\n",
      "SubGD iter. 484/499: loss=5.31062365013701, w0=72.62673267326743, w1=15.973177994657782\n",
      "SubGD iter. 485/499: loss=5.3105772079987075, w0=72.62673267326743, w1=15.971568333936695\n",
      "SubGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633673, w1=15.976037033744099\n",
      "SubGD iter. 487/499: loss=5.310625876282853, w0=72.62673267326743, w1=15.973910933958361\n",
      "SubGD iter. 488/499: loss=5.310578893403855, w0=72.62673267326743, w1=15.972301273237274\n",
      "SubGD iter. 489/499: loss=5.310575191964375, w0=72.62673267326743, w1=15.970691612516188\n",
      "SubGD iter. 490/499: loss=5.310583012749197, w0=72.63366336633673, w1=15.975160312323592\n",
      "SubGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326743, w1=15.973034212537854\n",
      "SubGD iter. 492/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.971424551816767\n",
      "SubGD iter. 493/499: loss=5.310578333769609, w0=72.63366336633673, w1=15.975893251624171\n",
      "SubGD iter. 494/499: loss=5.310625439575519, w0=72.62673267326743, w1=15.973767151838434\n",
      "SubGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.972157491117347\n",
      "SubGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.97054783039626\n",
      "SubGD iter. 497/499: loss=5.310583930633671, w0=72.63366336633673, w1=15.975016530203664\n",
      "SubGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326743, w1=15.972890430417927\n",
      "SubGD iter. 499/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.97128076969684\n",
      "SubGD: execution time=0.062 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc49158c6d0c49d1941d0ada3553802a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Compute a stochastic subgradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        for y_shuffle, tx_shuffle in batch_iter(y, tx, batch_size, num_batches=1, shuffle=True):\n",
    "            subgrad = compute_subgradient_mae(y, tx, w)\n",
    "            error = y - tx @ w\n",
    "            loss = calculate_mae(error)\n",
    "            \n",
    "            w = w - subgrad * gamma\n",
    "            \n",
    "            losses.append(loss)\n",
    "            ws.append(w)\n",
    "        \n",
    "        # ***************************************************\n",
    "        # raise NotImplementedError\n",
    "\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=8.756471895211877e-16\n",
      "SubSGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.7512943790423754e-15\n",
      "SubSGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=2.626941568563563e-15\n",
      "SubSGD iter. 3/499: loss=71.96780585492637, w0=2.8, w1=3.502588758084751e-15\n",
      "SubSGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=4.378235947605939e-15\n",
      "SubSGD iter. 5/499: loss=70.56780585492638, w0=4.2, w1=5.253883137127127e-15\n",
      "SubSGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=6.1295303266483146e-15\n",
      "SubSGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=7.0051775161695025e-15\n",
      "SubSGD iter. 8/499: loss=68.46780585492638, w0=6.300000000000001, w1=7.88082470569069e-15\n",
      "SubSGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=8.756471895211878e-15\n",
      "SubSGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000001, w1=9.632119084733065e-15\n",
      "SubSGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=1.0507766274254253e-14\n",
      "SubSGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=1.1383413463775441e-14\n",
      "SubSGD iter. 13/499: loss=64.96780585492637, w0=9.799999999999999, w1=1.2259060653296629e-14\n",
      "SubSGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=1.3134707842817817e-14\n",
      "SubSGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=1.4010355032339005e-14\n",
      "SubSGD iter. 16/499: loss=62.86780585492639, w0=11.899999999999997, w1=1.488600222186019e-14\n",
      "SubSGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.576164941138138e-14\n",
      "SubSGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.6637296600902567e-14\n",
      "SubSGD iter. 19/499: loss=60.767805854926394, w0=13.999999999999995, w1=1.7512943790423755e-14\n",
      "SubSGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.8388590979944943e-14\n",
      "SubSGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.926423816946613e-14\n",
      "SubSGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=2.013988535898732e-14\n",
      "SubSGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=2.1015532548508507e-14\n",
      "SubSGD iter. 24/499: loss=57.267805854926394, w0=17.499999999999993, w1=2.1891179738029695e-14\n",
      "SubSGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=2.2766826927550882e-14\n",
      "SubSGD iter. 26/499: loss=55.86780585492639, w0=18.89999999999999, w1=2.364247411707207e-14\n",
      "SubSGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=2.4518121306593258e-14\n",
      "SubSGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=2.5393768496114446e-14\n",
      "SubSGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=2.6269415685635634e-14\n",
      "SubSGD iter. 30/499: loss=53.067805854926384, w0=21.69999999999999, w1=2.7145062875156822e-14\n",
      "SubSGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=2.802071006467801e-14\n",
      "SubSGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.8896357254199195e-14\n",
      "SubSGD iter. 33/499: loss=50.96780585492638, w0=23.799999999999986, w1=2.977200444372038e-14\n",
      "SubSGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=3.064765163324157e-14\n",
      "SubSGD iter. 35/499: loss=49.567805854926405, w0=25.199999999999985, w1=3.152329882276276e-14\n",
      "SubSGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=3.2398946012283946e-14\n",
      "SubSGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=3.3274593201805134e-14\n",
      "SubSGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=3.415024039132632e-14\n",
      "SubSGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=3.502588758084751e-14\n",
      "SubSGD iter. 40/499: loss=46.06780585492639, w0=28.69999999999998, w1=3.59015347703687e-14\n",
      "SubSGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=3.6777181959889886e-14\n",
      "SubSGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=3.7652829149411074e-14\n",
      "SubSGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=3.852847633893226e-14\n",
      "SubSGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=3.940412352845345e-14\n",
      "SubSGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=4.027977071797464e-14\n",
      "SubSGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=4.1155417907495825e-14\n",
      "SubSGD iter. 47/499: loss=41.1678058549264, w0=33.59999999999999, w1=4.2031065097017013e-14\n",
      "SubSGD iter. 48/499: loss=40.4678058549264, w0=34.29999999999999, w1=4.29067122865382e-14\n",
      "SubSGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=4.378235947605939e-14\n",
      "SubSGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=4.465800666558058e-14\n",
      "SubSGD iter. 51/499: loss=38.36780585492639, w0=36.4, w1=4.5533653855101765e-14\n",
      "SubSGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=4.640930104462295e-14\n",
      "SubSGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=4.728494823414414e-14\n",
      "SubSGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=4.816059542366533e-14\n",
      "SubSGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=4.9036242613186517e-14\n",
      "SubSGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=4.9911889802707705e-14\n",
      "SubSGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=5.078753699222889e-14\n",
      "SubSGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=5.166318418175008e-14\n",
      "SubSGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=5.253883137127127e-14\n",
      "SubSGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=5.3414478560792456e-14\n",
      "SubSGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=5.4290125750313644e-14\n",
      "SubSGD iter. 62/499: loss=30.667805854926346, w0=44.10000000000003, w1=5.516577293983483e-14\n",
      "SubSGD iter. 63/499: loss=29.967805854926347, w0=44.80000000000003, w1=5.604142012935602e-14\n",
      "SubSGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=5.691706731887721e-14\n",
      "SubSGD iter. 65/499: loss=28.567805854926338, w0=46.20000000000004, w1=5.779271450839839e-14\n",
      "SubSGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=5.866836169791957e-14\n",
      "SubSGD iter. 67/499: loss=27.173270209668917, w0=47.59306930693074, w1=0.01114784567828894\n",
      "SubSGD iter. 68/499: loss=26.4904515637512, w0=48.279207920792125, w1=0.03308574108991741\n",
      "SubSGD iter. 69/499: loss=25.81721232277017, w0=48.96534653465351, w1=0.055023636501545875\n",
      "SubSGD iter. 70/499: loss=25.15503943465645, w0=49.63069306930698, w1=0.1053832638830964\n",
      "SubSGD iter. 71/499: loss=24.524103413894778, w0=50.28910891089114, w1=0.16746568532795278\n",
      "SubSGD iter. 72/499: loss=23.899295346035586, w0=50.947524752475296, w1=0.22954810677280915\n",
      "SubSGD iter. 73/499: loss=23.28439292565714, w0=51.59207920792084, w1=0.312425129327494\n",
      "SubSGD iter. 74/499: loss=22.68687644418184, w0=52.22277227722777, w1=0.41195013288401805\n",
      "SubSGD iter. 75/499: loss=22.10626756964055, w0=52.84653465346539, w1=0.5208167847923948\n",
      "SubSGD iter. 76/499: loss=21.53781882800843, w0=53.4564356435644, w1=0.6457900912636185\n",
      "SubSGD iter. 77/499: loss=20.986339874628463, w0=54.0594059405941, w1=0.7796904498577408\n",
      "SubSGD iter. 78/499: loss=20.445560936620446, w0=54.655445544554496, w1=0.9197570104995888\n",
      "SubSGD iter. 79/499: loss=19.91191015895784, w0=55.24455445544559, w1=1.067092029785011\n",
      "SubSGD iter. 80/499: loss=19.389644090563227, w0=55.819801980198065, w1=1.2261255948210965\n",
      "SubSGD iter. 81/499: loss=18.887989064395878, w0=56.36732673267331, w1=1.410709342622233\n",
      "SubSGD iter. 82/499: loss=18.41596050185423, w0=56.900990099009945, w1=1.605853732220289\n",
      "SubSGD iter. 83/499: loss=17.954898543040382, w0=57.42772277227727, w1=1.808762802293982\n",
      "SubSGD iter. 84/499: loss=17.505757656579817, w0=57.933663366336674, w1=2.0285064197514897\n",
      "SubSGD iter. 85/499: loss=17.074957426931608, w0=58.43267326732677, w1=2.2494370848672975\n",
      "SubSGD iter. 86/499: loss=16.652967297509893, w0=58.91089108910895, w1=2.4837982986028537\n",
      "SubSGD iter. 87/499: loss=16.248540731496718, w0=59.382178217821824, w1=2.7260245553531703\n",
      "SubSGD iter. 88/499: loss=15.849105212654152, w0=59.83960396039608, w1=2.978742333469156\n",
      "SubSGD iter. 89/499: loss=15.466919791231321, w0=60.262376237623805, w1=3.251528669355458\n",
      "SubSGD iter. 90/499: loss=15.108294621512211, w0=60.67821782178222, w1=3.5270865794243\n",
      "SubSGD iter. 91/499: loss=14.754896345922827, w0=61.087128712871326, w1=3.806459183951836\n",
      "SubSGD iter. 92/499: loss=14.404528961620272, w0=61.49603960396043, w1=4.085831788479371\n",
      "SubSGD iter. 93/499: loss=14.05578702812727, w0=61.891089108910926, w1=4.373839384328629\n",
      "SubSGD iter. 94/499: loss=13.714620911605627, w0=62.27920792079211, w1=4.666037469532069\n",
      "SubSGD iter. 95/499: loss=13.381236307284146, w0=62.65346534653469, w1=4.959829093241791\n",
      "SubSGD iter. 96/499: loss=13.058821615166227, w0=63.02079207920796, w1=5.257057192056662\n",
      "SubSGD iter. 97/499: loss=12.740251724339231, w0=63.38118811881192, w1=5.5604343163524295\n",
      "SubSGD iter. 98/499: loss=12.423218888756102, w0=63.74158415841588, w1=5.863811440648197\n",
      "SubSGD iter. 99/499: loss=12.107561731901159, w0=64.08811881188123, w1=6.172402175278572\n",
      "SubSGD iter. 100/499: loss=11.800622097398126, w0=64.42772277227726, w1=6.4863693105165225\n",
      "SubSGD iter. 101/499: loss=11.495041794646415, w0=64.7673267326733, w1=6.800336445754473\n",
      "SubSGD iter. 102/499: loss=11.189461491894704, w0=65.10693069306933, w1=7.1143035809924235\n",
      "SubSGD iter. 103/499: loss=10.883881189142992, w0=65.44653465346536, w1=7.428270716230374\n",
      "SubSGD iter. 104/499: loss=10.58459340831319, w0=65.76534653465349, w1=7.747893210218651\n",
      "SubSGD iter. 105/499: loss=10.295816534318933, w0=66.070297029703, w1=8.073669686866932\n",
      "SubSGD iter. 106/499: loss=10.01135208122135, w0=66.37524752475251, w1=8.399446163515213\n",
      "SubSGD iter. 107/499: loss=9.728084326668117, w0=66.6663366336634, w1=8.73297028041742\n",
      "SubSGD iter. 108/499: loss=9.448125461122496, w0=66.9574257425743, w1=9.066494397319628\n",
      "SubSGD iter. 109/499: loss=9.171041104096656, w0=67.23465346534658, w1=9.398630319470323\n",
      "SubSGD iter. 110/499: loss=8.903656131158947, w0=67.51188118811886, w1=9.730766241621017\n",
      "SubSGD iter. 111/499: loss=8.63627115822124, w0=67.78910891089114, w1=10.062902163771712\n",
      "SubSGD iter. 112/499: loss=8.376151920302359, w0=68.06633663366343, w1=10.363999289979459\n",
      "SubSGD iter. 113/499: loss=8.140540838751482, w0=68.32970297029709, w1=10.66046690927365\n",
      "SubSGD iter. 114/499: loss=7.918544501597259, w0=68.59306930693076, w1=10.943174379960851\n",
      "SubSGD iter. 115/499: loss=7.7052797283769845, w0=68.85643564356442, w1=11.225881850648053\n",
      "SubSGD iter. 116/499: loss=7.493695831178626, w0=69.11287128712878, w1=11.504395843582245\n",
      "SubSGD iter. 117/499: loss=7.2899924057434, w0=69.35544554455453, w1=11.78820189306779\n",
      "SubSGD iter. 118/499: loss=7.097234035781528, w0=69.58415841584166, w1=12.06091146519101\n",
      "SubSGD iter. 119/499: loss=6.919905294668907, w0=69.80594059405948, w1=12.324245668386087\n",
      "SubSGD iter. 120/499: loss=6.7505735273154395, w0=70.0277227722773, w1=12.587579871581164\n",
      "SubSGD iter. 121/499: loss=6.584744810805652, w0=70.25643564356443, w1=12.824765405096523\n",
      "SubSGD iter. 122/499: loss=6.4303432763477915, w0=70.47821782178225, w1=13.065616959310187\n",
      "SubSGD iter. 123/499: loss=6.27807148189034, w0=70.69306930693077, w1=13.302953389983951\n",
      "SubSGD iter. 124/499: loss=6.133663329263311, w0=70.89405940594067, w1=13.525403099312957\n",
      "SubSGD iter. 125/499: loss=6.005840798343018, w0=71.08811881188126, w1=13.742945617944251\n",
      "SubSGD iter. 126/499: loss=5.885021825223206, w0=71.27524752475254, w1=13.953548196006883\n",
      "SubSGD iter. 127/499: loss=5.771635252269647, w0=71.46237623762383, w1=14.164150774069515\n",
      "SubSGD iter. 128/499: loss=5.667162061790248, w0=71.62178217821788, w1=14.349779559473214\n",
      "SubSGD iter. 129/499: loss=5.586726765993136, w0=71.75346534653471, w1=14.516890107612351\n",
      "SubSGD iter. 130/499: loss=5.523847812160378, w0=71.87128712871292, w1=14.670791185324227\n",
      "SubSGD iter. 131/499: loss=5.480093708591866, w0=71.95445544554461, w1=14.780276456654562\n",
      "SubSGD iter. 132/499: loss=5.453088003502018, w0=72.0376237623763, w1=14.889761727984897\n",
      "SubSGD iter. 133/499: loss=5.4273926308629, w0=72.10693069306937, w1=14.985916181776767\n",
      "SubSGD iter. 134/499: loss=5.407322445682747, w0=72.17623762376245, w1=15.082070635568638\n",
      "SubSGD iter. 135/499: loss=5.387252260502595, w0=72.24554455445552, w1=15.178225089360508\n",
      "SubSGD iter. 136/499: loss=5.370460780338691, w0=72.30099009900998, w1=15.25972348971595\n",
      "SubSGD iter. 137/499: loss=5.3574065233347365, w0=72.34950495049513, w1=15.335091856448178\n",
      "SubSGD iter. 138/499: loss=5.345929264022579, w0=72.39801980198028, w1=15.410460223180406\n",
      "SubSGD iter. 139/499: loss=5.335714659517469, w0=72.43267326732682, w1=15.469961786755766\n",
      "SubSGD iter. 140/499: loss=5.330043910465359, w0=72.46039603960405, w1=15.51864528583285\n",
      "SubSGD iter. 141/499: loss=5.325676428273224, w0=72.48811881188128, w1=15.561592159086528\n",
      "SubSGD iter. 142/499: loss=5.322176726526588, w0=72.5019801980199, w1=15.597828332032567\n",
      "SubSGD iter. 143/499: loss=5.32011130964311, w0=72.52277227722782, w1=15.624722856626754\n",
      "SubSGD iter. 144/499: loss=5.318478284898438, w0=72.55049504950505, w1=15.642690329098041\n",
      "SubSGD iter. 145/499: loss=5.317240048565146, w0=72.56435643564366, w1=15.664356578291132\n",
      "SubSGD iter. 146/499: loss=5.316406547951545, w0=72.58514851485158, w1=15.677095775361325\n",
      "SubSGD iter. 147/499: loss=5.315557122666141, w0=72.6059405940595, w1=15.689834972431518\n",
      "SubSGD iter. 148/499: loss=5.31470769738074, w0=72.62673267326743, w1=15.70257416950171\n",
      "SubSGD iter. 149/499: loss=5.313876880922164, w0=72.64059405940604, w1=15.724240418694801\n",
      "SubSGD iter. 150/499: loss=5.313052246871382, w0=72.66138613861396, w1=15.736979615764994\n",
      "SubSGD iter. 151/499: loss=5.3123778390243865, w0=72.66831683168327, w1=15.74811029423132\n",
      "SubSGD iter. 152/499: loss=5.312132229725042, w0=72.67524752475258, w1=15.759240972697647\n",
      "SubSGD iter. 153/499: loss=5.311886620425695, w0=72.68217821782189, w1=15.770371651163973\n",
      "SubSGD iter. 154/499: loss=5.311683566098434, w0=72.68217821782189, w1=15.774323911906727\n",
      "SubSGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.77827617264948\n",
      "SubSGD iter. 156/499: loss=5.311638936484209, w0=72.68217821782189, w1=15.782228433392234\n",
      "SubSGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.786180694134988\n",
      "SubSGD iter. 158/499: loss=5.311594306869984, w0=72.68217821782189, w1=15.790132954877741\n",
      "SubSGD iter. 159/499: loss=5.311571992062872, w0=72.68217821782189, w1=15.794085215620495\n",
      "SubSGD iter. 160/499: loss=5.311549677255758, w0=72.68217821782189, w1=15.798037476363248\n",
      "SubSGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.801989737106002\n",
      "SubSGD iter. 162/499: loss=5.311505047641535, w0=72.68217821782189, w1=15.805941997848755\n",
      "SubSGD iter. 163/499: loss=5.3114827328344205, w0=72.68217821782189, w1=15.809894258591509\n",
      "SubSGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.813846519334263\n",
      "SubSGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.817798780077016\n",
      "SubSGD iter. 166/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.82175104081977\n",
      "SubSGD iter. 167/499: loss=5.311393473605971, w0=72.68217821782189, w1=15.825703301562523\n",
      "SubSGD iter. 168/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.829655562305277\n",
      "SubSGD iter. 169/499: loss=5.311348843991747, w0=72.68217821782189, w1=15.83360782304803\n",
      "SubSGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.837560083790784\n",
      "SubSGD iter. 171/499: loss=5.311304214377522, w0=72.68217821782189, w1=15.841512344533538\n",
      "SubSGD iter. 172/499: loss=5.31128189957041, w0=72.68217821782189, w1=15.845464605276291\n",
      "SubSGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.849416866019045\n",
      "SubSGD iter. 174/499: loss=5.311237269956185, w0=72.68217821782189, w1=15.853369126761798\n",
      "SubSGD iter. 175/499: loss=5.311214955149072, w0=72.68217821782189, w1=15.857321387504552\n",
      "SubSGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.861273648247305\n",
      "SubSGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.865225908990059\n",
      "SubSGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.869178169732812\n",
      "SubSGD iter. 179/499: loss=5.311125695920622, w0=72.68217821782189, w1=15.873130430475566\n",
      "SubSGD iter. 180/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.87708269121832\n",
      "SubSGD iter. 181/499: loss=5.311081066306398, w0=72.68217821782189, w1=15.881034951961073\n",
      "SubSGD iter. 182/499: loss=5.311058751499286, w0=72.68217821782189, w1=15.884987212703827\n",
      "SubSGD iter. 183/499: loss=5.311036436692172, w0=72.68217821782189, w1=15.88893947344658\n",
      "SubSGD iter. 184/499: loss=5.31101412188506, w0=72.68217821782189, w1=15.892891734189334\n",
      "SubSGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.896843994932087\n",
      "SubSGD iter. 186/499: loss=5.310969492270836, w0=72.68217821782189, w1=15.900796255674841\n",
      "SubSGD iter. 187/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.904748516417595\n",
      "SubSGD iter. 188/499: loss=5.310924862656612, w0=72.68217821782189, w1=15.908700777160348\n",
      "SubSGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.912653037903102\n",
      "SubSGD iter. 190/499: loss=5.310913706061381, w0=72.67524752475258, w1=15.910526938117364\n",
      "SubSGD iter. 191/499: loss=5.310892237186269, w0=72.67524752475258, w1=15.914479198860118\n",
      "SubSGD iter. 192/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.918431459602871\n",
      "SubSGD iter. 193/499: loss=5.310862636053835, w0=72.66831683168327, w1=15.916305359817134\n",
      "SubSGD iter. 194/499: loss=5.310859611715928, w0=72.66831683168327, w1=15.920257620559887\n",
      "SubSGD iter. 195/499: loss=5.310837296908815, w0=72.66831683168327, w1=15.924209881302641\n",
      "SubSGD iter. 196/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.928162142045394\n",
      "SubSGD iter. 197/499: loss=5.310823570190172, w0=72.66138613861396, w1=15.926036042259657\n",
      "SubSGD iter. 198/499: loss=5.310804671438475, w0=72.66138613861396, w1=15.92998830300241\n",
      "SubSGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.933940563745164\n",
      "SubSGD iter. 200/499: loss=5.310772500182623, w0=72.65445544554466, w1=15.931814463959427\n",
      "SubSGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.93576672470218\n",
      "SubSGD iter. 202/499: loss=5.310749731161019, w0=72.65445544554466, w1=15.939718985444934\n",
      "SubSGD iter. 203/499: loss=5.310727416353907, w0=72.65445544554466, w1=15.943671246187687\n",
      "SubSGD iter. 204/499: loss=5.310733434318959, w0=72.64752475247535, w1=15.94154514640195\n",
      "SubSGD iter. 205/499: loss=5.310717105690678, w0=72.64752475247535, w1=15.945497407144703\n",
      "SubSGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.949449667887457\n",
      "SubSGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940604, w1=15.94732356810172\n",
      "SubSGD iter. 208/499: loss=5.310684480220336, w0=72.64059405940604, w1=15.951275828844473\n",
      "SubSGD iter. 209/499: loss=5.310662165413224, w0=72.64059405940604, w1=15.955228089587226\n",
      "SubSGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.95918035032998\n",
      "SubSGD iter. 211/499: loss=5.310643298447746, w0=72.63366336633673, w1=15.957054250544243\n",
      "SubSGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.961006511286996\n",
      "SubSGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.96495877202975\n",
      "SubSGD iter. 214/499: loss=5.3105922284402, w0=72.62673267326743, w1=15.962832672244012\n",
      "SubSGD iter. 215/499: loss=5.310633183099026, w0=72.63366336633673, w1=15.967301372051416\n",
      "SubSGD iter. 216/499: loss=5.3105993435850625, w0=72.62673267326743, w1=15.965175272265679\n",
      "SubSGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633673, w1=15.969643972073083\n",
      "SubSGD iter. 218/499: loss=5.310606458729926, w0=72.62673267326743, w1=15.967517872287345\n",
      "SubSGD iter. 219/499: loss=5.3106032734525535, w0=72.63366336633673, w1=15.97198657209475\n",
      "SubSGD iter. 220/499: loss=5.3106135738747895, w0=72.62673267326743, w1=15.969860472309012\n",
      "SubSGD iter. 221/499: loss=5.310588318629316, w0=72.63366336633673, w1=15.974329172116416\n",
      "SubSGD iter. 222/499: loss=5.310620689019653, w0=72.62673267326743, w1=15.972203072330679\n",
      "SubSGD iter. 223/499: loss=5.310574966149885, w0=72.62673267326743, w1=15.970593411609592\n",
      "SubSGD iter. 224/499: loss=5.31058363964973, w0=72.63366336633673, w1=15.975062111416996\n",
      "SubSGD iter. 225/499: loss=5.310622915165494, w0=72.62673267326743, w1=15.972936011631258\n",
      "SubSGD iter. 226/499: loss=5.310576651555034, w0=72.62673267326743, w1=15.971326350910171\n",
      "SubSGD iter. 227/499: loss=5.310578960670142, w0=72.63366336633673, w1=15.975795050717576\n",
      "SubSGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326743, w1=15.973668950931838\n",
      "SubSGD iter. 229/499: loss=5.31057833696018, w0=72.62673267326743, w1=15.972059290210751\n",
      "SubSGD iter. 230/499: loss=5.310574635520698, w0=72.62673267326743, w1=15.970449629489664\n",
      "SubSGD iter. 231/499: loss=5.310584557534202, w0=72.63366336633673, w1=15.974918329297068\n",
      "SubSGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326743, w1=15.972792229511331\n",
      "SubSGD iter. 233/499: loss=5.310576320925845, w0=72.62673267326743, w1=15.971182568790244\n",
      "SubSGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633673, w1=15.975651268597648\n",
      "SubSGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326743, w1=15.97352516881191\n",
      "SubSGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.971915508090824\n",
      "SubSGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633673, w1=15.976384207898228\n",
      "SubSGD iter. 238/499: loss=5.310626930749844, w0=72.62673267326743, w1=15.97425810811249\n",
      "SubSGD iter. 239/499: loss=5.310579691736141, w0=72.62673267326743, w1=15.972648447391403\n",
      "SubSGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.971038786670317\n",
      "SubSGD iter. 241/499: loss=5.310580796439089, w0=72.63366336633673, w1=15.97550748647772\n",
      "SubSGD iter. 242/499: loss=5.310624267896666, w0=72.62673267326743, w1=15.973381386691983\n",
      "SubSGD iter. 243/499: loss=5.310577675701806, w0=72.62673267326743, w1=15.971771725970896\n",
      "SubSGD iter. 244/499: loss=5.3105761174595, w0=72.63366336633673, w1=15.9762404257783\n",
      "SubSGD iter. 245/499: loss=5.310626494042512, w0=72.62673267326743, w1=15.974114325992563\n",
      "SubSGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.972504665271476\n",
      "SubSGD iter. 247/499: loss=5.310575659667473, w0=72.62673267326743, w1=15.970895004550389\n",
      "SubSGD iter. 248/499: loss=5.310581714323562, w0=72.63366336633673, w1=15.975363704357793\n",
      "SubSGD iter. 249/499: loss=5.310623831189333, w0=72.62673267326743, w1=15.973237604572056\n",
      "SubSGD iter. 250/499: loss=5.310577345072619, w0=72.62673267326743, w1=15.971627943850969\n",
      "SubSGD iter. 251/499: loss=5.310577035343974, w0=72.63366336633673, w1=15.976096643658373\n",
      "SubSGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326743, w1=15.973970543872635\n",
      "SubSGD iter. 253/499: loss=5.310579030477768, w0=72.62673267326743, w1=15.972360883151548\n",
      "SubSGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.970751222430462\n",
      "SubSGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633673, w1=15.975219922237866\n",
      "SubSGD iter. 256/499: loss=5.310623394482, w0=72.62673267326743, w1=15.973093822452128\n",
      "SubSGD iter. 257/499: loss=5.310577014443433, w0=72.62673267326743, w1=15.971484161731041\n",
      "SubSGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633673, w1=15.975952861538445\n",
      "SubSGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326743, w1=15.973826761752708\n",
      "SubSGD iter. 260/499: loss=5.310578699848579, w0=72.62673267326743, w1=15.972217101031621\n",
      "SubSGD iter. 261/499: loss=5.310574998409098, w0=72.62673267326743, w1=15.970607440310534\n",
      "SubSGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633673, w1=15.975076140117938\n",
      "SubSGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326743, w1=15.9729500403322\n",
      "SubSGD iter. 264/499: loss=5.310576683814246, w0=72.62673267326743, w1=15.971340379611114\n",
      "SubSGD iter. 265/499: loss=5.310578871112923, w0=72.63366336633673, w1=15.975809079418518\n",
      "SubSGD iter. 266/499: loss=5.310625183920507, w0=72.62673267326743, w1=15.97368297963278\n",
      "SubSGD iter. 267/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.972073318911693\n",
      "SubSGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.970463658190607\n",
      "SubSGD iter. 269/499: loss=5.310584467976983, w0=72.63366336633673, w1=15.97493235799801\n",
      "SubSGD iter. 270/499: loss=5.310622521067329, w0=72.62673267326743, w1=15.972806258212273\n",
      "SubSGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.971196597491186\n",
      "SubSGD iter. 272/499: loss=5.310579788997396, w0=72.63366336633673, w1=15.97566529729859\n",
      "SubSGD iter. 273/499: loss=5.310624747213171, w0=72.62673267326743, w1=15.973539197512853\n",
      "SubSGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.971929536791766\n",
      "SubSGD iter. 275/499: loss=5.310575110017808, w0=72.63366336633673, w1=15.97639823659917\n",
      "SubSGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326743, w1=15.974272136813433\n",
      "SubSGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.972662476092346\n",
      "SubSGD iter. 278/499: loss=5.310576022555872, w0=72.62673267326743, w1=15.971052815371259\n",
      "SubSGD iter. 279/499: loss=5.31058070688187, w0=72.63366336633673, w1=15.975521515178663\n",
      "SubSGD iter. 280/499: loss=5.310624310505836, w0=72.62673267326743, w1=15.973395415392925\n",
      "SubSGD iter. 281/499: loss=5.310577707961019, w0=72.62673267326743, w1=15.971785754671838\n",
      "SubSGD iter. 282/499: loss=5.3105760279022824, w0=72.63366336633673, w1=15.976254454479243\n",
      "SubSGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326743, w1=15.974128354693505\n",
      "SubSGD iter. 284/499: loss=5.310579393366167, w0=72.62673267326743, w1=15.972518693972418\n",
      "SubSGD iter. 285/499: loss=5.310575691926685, w0=72.62673267326743, w1=15.970909033251331\n",
      "SubSGD iter. 286/499: loss=5.3105816247663435, w0=72.63366336633673, w1=15.975377733058735\n",
      "SubSGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326743, w1=15.973251633272998\n",
      "SubSGD iter. 288/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.971641972551911\n",
      "SubSGD iter. 289/499: loss=5.310576945786756, w0=72.63366336633673, w1=15.976110672359315\n",
      "SubSGD iter. 290/499: loss=5.310626099944345, w0=72.62673267326743, w1=15.973984572573578\n",
      "SubSGD iter. 291/499: loss=5.310579062736981, w0=72.62673267326743, w1=15.97237491185249\n",
      "SubSGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.970765251131404\n",
      "SubSGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633673, w1=15.975233950938808\n",
      "SubSGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326743, w1=15.97310785115307\n",
      "SubSGD iter. 295/499: loss=5.310577046702646, w0=72.62673267326743, w1=15.971498190431983\n",
      "SubSGD iter. 296/499: loss=5.31057786367123, w0=72.63366336633673, w1=15.975966890239388\n",
      "SubSGD iter. 297/499: loss=5.310625663237009, w0=72.62673267326743, w1=15.97384079045365\n",
      "SubSGD iter. 298/499: loss=5.310578732107793, w0=72.62673267326743, w1=15.972231129732563\n",
      "SubSGD iter. 299/499: loss=5.310575030668311, w0=72.62673267326743, w1=15.970621469011476\n",
      "SubSGD iter. 300/499: loss=5.31058346053529, w0=72.63366336633673, w1=15.97509016881888\n",
      "SubSGD iter. 301/499: loss=5.310623000383833, w0=72.62673267326743, w1=15.972964069033143\n",
      "SubSGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.971354408312056\n",
      "SubSGD iter. 303/499: loss=5.310578781555704, w0=72.63366336633673, w1=15.97582310811946\n",
      "SubSGD iter. 304/499: loss=5.310625226529675, w0=72.62673267326743, w1=15.973697008333723\n",
      "SubSGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.972087347612636\n",
      "SubSGD iter. 306/499: loss=5.310574700039124, w0=72.62673267326743, w1=15.970477686891549\n",
      "SubSGD iter. 307/499: loss=5.310584378419764, w0=72.63366336633673, w1=15.974946386698953\n",
      "SubSGD iter. 308/499: loss=5.310622563676497, w0=72.62673267326743, w1=15.972820286913215\n",
      "SubSGD iter. 309/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.971210626192129\n",
      "SubSGD iter. 310/499: loss=5.310579699440178, w0=72.63366336633673, w1=15.975679325999533\n",
      "SubSGD iter. 311/499: loss=5.310624789822341, w0=72.62673267326743, w1=15.973553226213795\n",
      "SubSGD iter. 312/499: loss=5.310578070849419, w0=72.62673267326743, w1=15.971943565492708\n",
      "SubSGD iter. 313/499: loss=5.310575020460591, w0=72.63366336633673, w1=15.976412265300112\n",
      "SubSGD iter. 314/499: loss=5.310627015968183, w0=72.62673267326743, w1=15.974286165514375\n",
      "SubSGD iter. 315/499: loss=5.310579756254565, w0=72.62673267326743, w1=15.972676504793288\n",
      "SubSGD iter. 316/499: loss=5.310576054815084, w0=72.62673267326743, w1=15.971066844072201\n",
      "SubSGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633673, w1=15.975535543879605\n",
      "SubSGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326743, w1=15.973409444093868\n",
      "SubSGD iter. 319/499: loss=5.310577740220233, w0=72.62673267326743, w1=15.97179978337278\n",
      "SubSGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633673, w1=15.976268483180185\n",
      "SubSGD iter. 321/499: loss=5.310626579260847, w0=72.62673267326743, w1=15.974142383394447\n",
      "SubSGD iter. 322/499: loss=5.31057942562538, w0=72.62673267326743, w1=15.97253272267336\n",
      "SubSGD iter. 323/499: loss=5.310575724185898, w0=72.62673267326743, w1=15.970923061952274\n",
      "SubSGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633673, w1=15.975391761759678\n",
      "SubSGD iter. 325/499: loss=5.310623916407669, w0=72.62673267326743, w1=15.97326566197394\n",
      "SubSGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.971656001252853\n",
      "SubSGD iter. 327/499: loss=5.310576856229536, w0=72.63366336633673, w1=15.976124701060257\n",
      "SubSGD iter. 328/499: loss=5.310626142553513, w0=72.62673267326743, w1=15.97399860127452\n",
      "SubSGD iter. 329/499: loss=5.310579094996192, w0=72.62673267326743, w1=15.972388940553433\n",
      "SubSGD iter. 330/499: loss=5.31057539355671, w0=72.62673267326743, w1=15.970779279832346\n",
      "SubSGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633673, w1=15.97524797963975\n",
      "SubSGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326743, w1=15.973121879854013\n",
      "SubSGD iter. 333/499: loss=5.310577078961858, w0=72.62673267326743, w1=15.971512219132926\n",
      "SubSGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633673, w1=15.97598091894033\n",
      "SubSGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326743, w1=15.973854819154592\n",
      "SubSGD iter. 336/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.972245158433505\n",
      "SubSGD iter. 337/499: loss=5.310575062927524, w0=72.62673267326743, w1=15.970635497712419\n",
      "SubSGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633673, w1=15.975104197519823\n",
      "SubSGD iter. 339/499: loss=5.310623042993, w0=72.62673267326743, w1=15.972978097734085\n",
      "SubSGD iter. 340/499: loss=5.310576748332672, w0=72.62673267326743, w1=15.971368437012998\n",
      "SubSGD iter. 341/499: loss=5.310578691998485, w0=72.63366336633673, w1=15.975837136820402\n",
      "SubSGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326743, w1=15.973711037034665\n",
      "SubSGD iter. 343/499: loss=5.310578433737819, w0=72.62673267326743, w1=15.972101376313578\n",
      "SubSGD iter. 344/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.970491715592491\n",
      "SubSGD iter. 345/499: loss=5.310584288862545, w0=72.63366336633673, w1=15.974960415399895\n",
      "SubSGD iter. 346/499: loss=5.3106226062856665, w0=72.62673267326743, w1=15.972834315614158\n",
      "SubSGD iter. 347/499: loss=5.3105764177034835, w0=72.62673267326743, w1=15.97122465489307\n",
      "SubSGD iter. 348/499: loss=5.310579609882959, w0=72.63366336633673, w1=15.975693354700475\n",
      "SubSGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326743, w1=15.973567254914737\n",
      "SubSGD iter. 350/499: loss=5.3105781031086305, w0=72.62673267326743, w1=15.97195759419365\n",
      "SubSGD iter. 351/499: loss=5.310574930903369, w0=72.63366336633673, w1=15.976426294001055\n",
      "SubSGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326743, w1=15.974300194215317\n",
      "SubSGD iter. 353/499: loss=5.310579788513779, w0=72.62673267326743, w1=15.97269053349423\n",
      "SubSGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.971080872773143\n",
      "SubSGD iter. 355/499: loss=5.310580527767431, w0=72.63366336633673, w1=15.975549572580547\n",
      "SubSGD iter. 356/499: loss=5.310624395724175, w0=72.62673267326743, w1=15.97342347279481\n",
      "SubSGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.971813812073723\n",
      "SubSGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633673, w1=15.976282511881127\n",
      "SubSGD iter. 359/499: loss=5.310626621870016, w0=72.62673267326743, w1=15.97415641209539\n",
      "SubSGD iter. 360/499: loss=5.310579457884592, w0=72.62673267326743, w1=15.972546751374303\n",
      "SubSGD iter. 361/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.970937090653216\n",
      "SubSGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633673, w1=15.97540579046062\n",
      "SubSGD iter. 363/499: loss=5.310623959016838, w0=72.62673267326743, w1=15.973279690674882\n",
      "SubSGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.971670029953795\n",
      "SubSGD iter. 365/499: loss=5.310576766672318, w0=72.63366336633673, w1=15.9761387297612\n",
      "SubSGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326743, w1=15.974012629975462\n",
      "SubSGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.972402969254375\n",
      "SubSGD iter. 368/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.970793308533288\n",
      "SubSGD iter. 369/499: loss=5.310582363536379, w0=72.63366336633673, w1=15.975262008340692\n",
      "SubSGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326743, w1=15.973135908554955\n",
      "SubSGD iter. 371/499: loss=5.310577111221071, w0=72.62673267326743, w1=15.971526247833868\n",
      "SubSGD iter. 372/499: loss=5.310577684556792, w0=72.63366336633673, w1=15.975994947641272\n",
      "SubSGD iter. 373/499: loss=5.310625748455347, w0=72.62673267326743, w1=15.973868847855535\n",
      "SubSGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.972259187134448\n",
      "SubSGD iter. 375/499: loss=5.310575095186737, w0=72.62673267326743, w1=15.97064952641336\n",
      "SubSGD iter. 376/499: loss=5.310583281420854, w0=72.63366336633673, w1=15.975118226220765\n",
      "SubSGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326743, w1=15.972992126435027\n",
      "SubSGD iter. 378/499: loss=5.310576780591883, w0=72.62673267326743, w1=15.97138246571394\n",
      "SubSGD iter. 379/499: loss=5.310578602441266, w0=72.63366336633673, w1=15.975851165521345\n",
      "SubSGD iter. 380/499: loss=5.310625311748013, w0=72.62673267326743, w1=15.973725065735607\n",
      "SubSGD iter. 381/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.97211540501452\n",
      "SubSGD iter. 382/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.970505744293433\n",
      "SubSGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633673, w1=15.974974444100837\n",
      "SubSGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326743, w1=15.9728483443151\n",
      "SubSGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.971238683594013\n",
      "SubSGD iter. 386/499: loss=5.310579520325739, w0=72.63366336633673, w1=15.975707383401417\n",
      "SubSGD iter. 387/499: loss=5.310624875040677, w0=72.62673267326743, w1=15.97358128361568\n",
      "SubSGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.971971622894593\n",
      "SubSGD iter. 389/499: loss=5.310574841346153, w0=72.63366336633673, w1=15.976440322701997\n",
      "SubSGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326743, w1=15.97431422291626\n",
      "SubSGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.972704562195172\n",
      "SubSGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.971094901474086\n",
      "SubSGD iter. 393/499: loss=5.310580438210212, w0=72.63366336633673, w1=15.97556360128149\n",
      "SubSGD iter. 394/499: loss=5.310624438333342, w0=72.62673267326743, w1=15.973437501495752\n",
      "SubSGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.971827840774665\n",
      "SubSGD iter. 396/499: loss=5.310575759230625, w0=72.63366336633673, w1=15.97629654058207\n",
      "SubSGD iter. 397/499: loss=5.3106266644791855, w0=72.62673267326743, w1=15.974170440796332\n",
      "SubSGD iter. 398/499: loss=5.310579490143805, w0=72.62673267326743, w1=15.972560780075245\n",
      "SubSGD iter. 399/499: loss=5.310575788704324, w0=72.62673267326743, w1=15.970951119354158\n",
      "SubSGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633673, w1=15.975419819161562\n",
      "SubSGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326743, w1=15.973293719375825\n",
      "SubSGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.971684058654738\n",
      "SubSGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633673, w1=15.976152758462142\n",
      "SubSGD iter. 404/499: loss=5.31062622777185, w0=72.62673267326743, w1=15.974026658676404\n",
      "SubSGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.972416997955317\n",
      "SubSGD iter. 406/499: loss=5.310575458075137, w0=72.62673267326743, w1=15.97080733723423\n",
      "SubSGD iter. 407/499: loss=5.31058227397916, w0=72.63366336633673, w1=15.975276037041635\n",
      "SubSGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326743, w1=15.973149937255897\n",
      "SubSGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.97154027653481\n",
      "SubSGD iter. 410/499: loss=5.310577594999573, w0=72.63366336633673, w1=15.976008976342214\n",
      "SubSGD iter. 411/499: loss=5.310625791064515, w0=72.62673267326743, w1=15.973882876556477\n",
      "SubSGD iter. 412/499: loss=5.310578828885431, w0=72.62673267326743, w1=15.97227321583539\n",
      "SubSGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.970663555114303\n",
      "SubSGD iter. 414/499: loss=5.310583191863635, w0=72.63366336633673, w1=15.975132254921707\n",
      "SubSGD iter. 415/499: loss=5.310623128211339, w0=72.62673267326743, w1=15.97300615513597\n",
      "SubSGD iter. 416/499: loss=5.310576812851096, w0=72.62673267326743, w1=15.971396494414883\n",
      "SubSGD iter. 417/499: loss=5.310578512884047, w0=72.63366336633673, w1=15.975865194222287\n",
      "SubSGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326743, w1=15.97373909443655\n",
      "SubSGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.972129433715462\n",
      "SubSGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.970519772994376\n",
      "SubSGD iter. 421/499: loss=5.310584109748109, w0=72.63366336633673, w1=15.97498847280178\n",
      "SubSGD iter. 422/499: loss=5.310622691504003, w0=72.62673267326743, w1=15.972862373016042\n",
      "SubSGD iter. 423/499: loss=5.31057648222191, w0=72.62673267326743, w1=15.971252712294955\n",
      "SubSGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633673, w1=15.97572141210236\n",
      "SubSGD iter. 425/499: loss=5.310624917649846, w0=72.62673267326743, w1=15.973595312316622\n",
      "SubSGD iter. 426/499: loss=5.310578167627058, w0=72.62673267326743, w1=15.971985651595535\n",
      "SubSGD iter. 427/499: loss=5.310574751788933, w0=72.63366336633673, w1=15.97645435140294\n",
      "SubSGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326743, w1=15.974328251617202\n",
      "SubSGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.972718590896115\n",
      "SubSGD iter. 430/499: loss=5.3105761515927234, w0=72.62673267326743, w1=15.971108930175028\n",
      "SubSGD iter. 431/499: loss=5.310580348652993, w0=72.63366336633673, w1=15.975577629982432\n",
      "SubSGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326743, w1=15.973451530196694\n",
      "SubSGD iter. 433/499: loss=5.310577836997869, w0=72.62673267326743, w1=15.971841869475607\n",
      "SubSGD iter. 434/499: loss=5.310575669673406, w0=72.63366336633673, w1=15.976310569283012\n",
      "SubSGD iter. 435/499: loss=5.310626707088355, w0=72.62673267326743, w1=15.974184469497274\n",
      "SubSGD iter. 436/499: loss=5.310579522403018, w0=72.62673267326743, w1=15.972574808776187\n",
      "SubSGD iter. 437/499: loss=5.310575820963535, w0=72.62673267326743, w1=15.9709651480551\n",
      "SubSGD iter. 438/499: loss=5.3105812665374685, w0=72.63366336633673, w1=15.975433847862504\n",
      "SubSGD iter. 439/499: loss=5.310624044235176, w0=72.62673267326743, w1=15.973307748076767\n",
      "SubSGD iter. 440/499: loss=5.310577506368682, w0=72.62673267326743, w1=15.97169808735568\n",
      "SubSGD iter. 441/499: loss=5.310576587557881, w0=72.63366336633673, w1=15.976166787163084\n",
      "SubSGD iter. 442/499: loss=5.310626270381018, w0=72.62673267326743, w1=15.974040687377347\n",
      "SubSGD iter. 443/499: loss=5.310579191773829, w0=72.62673267326743, w1=15.97243102665626\n",
      "SubSGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.970821365935173\n",
      "SubSGD iter. 445/499: loss=5.310582184421942, w0=72.63366336633673, w1=15.975290065742577\n",
      "SubSGD iter. 446/499: loss=5.310623607527842, w0=72.62673267326743, w1=15.97316396595684\n",
      "SubSGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.971554305235752\n",
      "SubSGD iter. 448/499: loss=5.310577505442354, w0=72.63366336633673, w1=15.976023005043157\n",
      "SubSGD iter. 449/499: loss=5.310625833673685, w0=72.62673267326743, w1=15.97389690525742\n",
      "SubSGD iter. 450/499: loss=5.310578861144642, w0=72.62673267326743, w1=15.972287244536332\n",
      "SubSGD iter. 451/499: loss=5.310575159705164, w0=72.62673267326743, w1=15.970677583815245\n",
      "SubSGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633673, w1=15.97514628362265\n",
      "SubSGD iter. 453/499: loss=5.310623170820506, w0=72.62673267326743, w1=15.973020183836912\n",
      "SubSGD iter. 454/499: loss=5.31057684511031, w0=72.62673267326743, w1=15.971410523115825\n",
      "SubSGD iter. 455/499: loss=5.310578423326829, w0=72.63366336633673, w1=15.97587922292323\n",
      "SubSGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326743, w1=15.973753123137492\n",
      "SubSGD iter. 457/499: loss=5.310578530515457, w0=72.62673267326743, w1=15.972143462416405\n",
      "SubSGD iter. 458/499: loss=5.310574829075975, w0=72.62673267326743, w1=15.970533801695318\n",
      "SubSGD iter. 459/499: loss=5.31058402019089, w0=72.63366336633673, w1=15.975002501502722\n",
      "SubSGD iter. 460/499: loss=5.3106227341131715, w0=72.62673267326743, w1=15.972876401716984\n",
      "SubSGD iter. 461/499: loss=5.310576514481121, w0=72.62673267326743, w1=15.971266740995897\n",
      "SubSGD iter. 462/499: loss=5.310579341211301, w0=72.63366336633673, w1=15.975735440803302\n",
      "SubSGD iter. 463/499: loss=5.310624960259015, w0=72.62673267326743, w1=15.973609341017564\n",
      "SubSGD iter. 464/499: loss=5.310578199886269, w0=72.62673267326743, w1=15.971999680296477\n",
      "SubSGD iter. 465/499: loss=5.310574662231715, w0=72.63366336633673, w1=15.976468380103881\n",
      "SubSGD iter. 466/499: loss=5.310627186404856, w0=72.62673267326743, w1=15.974342280318144\n",
      "SubSGD iter. 467/499: loss=5.310579885291418, w0=72.62673267326743, w1=15.972732619597057\n",
      "SubSGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.97112295887597\n",
      "SubSGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633673, w1=15.975591658683374\n",
      "SubSGD iter. 470/499: loss=5.31062452355168, w0=72.62673267326743, w1=15.973465558897637\n",
      "SubSGD iter. 471/499: loss=5.310577869257082, w0=72.62673267326743, w1=15.97185589817655\n",
      "SubSGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633673, w1=15.976324597983954\n",
      "SubSGD iter. 473/499: loss=5.310626749697523, w0=72.62673267326743, w1=15.974198498198216\n",
      "SubSGD iter. 474/499: loss=5.310579554662228, w0=72.62673267326743, w1=15.97258883747713\n",
      "SubSGD iter. 475/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.970979176756043\n",
      "SubSGD iter. 476/499: loss=5.310581176980249, w0=72.63366336633673, w1=15.975447876563447\n",
      "SubSGD iter. 477/499: loss=5.310624086844345, w0=72.62673267326743, w1=15.97332177677771\n",
      "SubSGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.971712116056622\n",
      "SubSGD iter. 479/499: loss=5.310576498000661, w0=72.63366336633673, w1=15.976180815864026\n",
      "SubSGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326743, w1=15.974054716078289\n",
      "SubSGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.972445055357202\n",
      "SubSGD iter. 482/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.970835394636115\n",
      "SubSGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633673, w1=15.97530409444352\n",
      "SubSGD iter. 484/499: loss=5.31062365013701, w0=72.62673267326743, w1=15.973177994657782\n",
      "SubSGD iter. 485/499: loss=5.3105772079987075, w0=72.62673267326743, w1=15.971568333936695\n",
      "SubSGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633673, w1=15.976037033744099\n",
      "SubSGD iter. 487/499: loss=5.310625876282853, w0=72.62673267326743, w1=15.973910933958361\n",
      "SubSGD iter. 488/499: loss=5.310578893403855, w0=72.62673267326743, w1=15.972301273237274\n",
      "SubSGD iter. 489/499: loss=5.310575191964375, w0=72.62673267326743, w1=15.970691612516188\n",
      "SubSGD iter. 490/499: loss=5.310583012749197, w0=72.63366336633673, w1=15.975160312323592\n",
      "SubSGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326743, w1=15.973034212537854\n",
      "SubSGD iter. 492/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.971424551816767\n",
      "SubSGD iter. 493/499: loss=5.310578333769609, w0=72.63366336633673, w1=15.975893251624171\n",
      "SubSGD iter. 494/499: loss=5.310625439575519, w0=72.62673267326743, w1=15.973767151838434\n",
      "SubSGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.972157491117347\n",
      "SubSGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.97054783039626\n",
      "SubSGD iter. 497/499: loss=5.310583930633671, w0=72.63366336633673, w1=15.975016530203664\n",
      "SubSGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326743, w1=15.972890430417927\n",
      "SubSGD iter. 499/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.97128076969684\n",
      "SubSGD: execution time=0.096 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef65d059cb34da0aa05cdec2b0ce9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
